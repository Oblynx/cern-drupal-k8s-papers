
@inproceedings{megino_using_2020,
	title = {Using {Kubernetes} as an {ATLAS} computing site},
	doi = {10.1051/epjconf/202024507025},
	abstract = {In recent years containerization has revolutionized cloud environments, providing a secure, lightweight, standardized way to package and execute software. Solutions such as Kubernetes enable orchestration of containers in a cluster, including for the purpose of job scheduling. Kubernetes is becoming a de facto standard, available at all major cloud computing providers, and is gaining increased attention from some WLCG sites. In particular, CERN IT has integrated Kubernetes into their cloud infrastructure by providing an interface to instantly create Kubernetes clusters, and the University of Victoria is pursuing an infrastructure-as-code approach to deploying Kubernetes as a flexible and resilient platform for running services and delivering resources.The ATLAS experiment at the LHC has partnered with CERN IT and the University of Victoria to explore and demonstrate the feasibility of running an ATLAS computing site directly on Kubernetes, replacing all grid computing services. We have interfaced ATLAS’ workload submission engine PanDA with Kubernetes, to directly submit and monitor the status of containerized jobs. We describe the integration and deployment details, and focus on the lessons learned from running a wide variety of ATLAS production payloads on Kubernetes using clusters of several thousand cores at CERN and the Tier 2 computing site in Victoria.},
	author = {Megino, F. B. and Donell, D. M. M. and Seuster, R. and Taylor, R. P. and Berghaus, F. and Maeno, T. and Albert, J. and De, K. and Lin, F. and Rocha, R. B. D. and Yang, Ming-Jyuan},
	year = {2020},
}

@article{kaviani_towards_2019,
	title = {Towards {Serverless} as {Commodity}: a case of {Knative}},
	shorttitle = {Towards {Serverless} as {Commodity}},
	doi = {10.1145/3366623.3368135},
	abstract = {Serverless computing promises to evolve cloud computing architecture from VMs and containers-as-a-service (CaaS) to function-as-a-service (FaaS). This takes away complexities of managing and scaling underlying infrastructure and can result in simpler code, cheaper realization of services, and higher availability. Nonetheless, one of the primary drawbacks customers face when making decision to move their software to a serverless platform is the potential for getting locked-in with a particular provider. This used to be a concern with Platform-as-a-Service (PaaS) offerings too. However with Kubernetes emerging as the industry standard PaaS layer, PaaS is closer to becoming commodity with the Kubernetes API as its common interface. The question is if a similar unification for the API interface layer and runtime contracts can be achieved for serverless. If achieved, this would free up serverless users from their fears of platform lock-in. Our goal in this paper is to extract a minimal common denominator model of execution that can move us closer to a unified serverless platform. As contributors to Knative [13] with in-depth understanding of its internal design, we use Knative as the baseline for this comparison and contrast its API interface and runtime contracts against other prominent serverless platforms to identify commonalities and differences. Influenced by the work in Knative, we also discuss challenges as well as the necessary evolution we expect to see as serverless platforms themselves reach commodity status.},
	journal = {WOSC@Middleware},
	author = {Kaviani, N. and Kalinin, D. and Maximilien, E.},
	year = {2019},
}

@article{shaw_germinate_2017,
	title = {Germinate 3 : development of a common platform to support the distribution of experimental data on crop wild relatives},
	shorttitle = {Germinate 3},
	doi = {10.2135/CROPSCI2016.09.0814},
	abstract = {Conservation and exploitation of crop wild relative species is an important component in ensuring food security and improving current agricultural output. By identifying agriculturally important characteristics that express favorable response to both biotic and abiotic stress currently unused by breeders, the incorporation of this new genetic material into genetic background stocks may help mitigate problems imposed by climate change, land degradation, and population pressure. This is particularly important in countries that will be more severely affected by the threat of reduced yields. The ability to effectively manage genetic resources collections and integrate unique and diverse data types is crucial in exploring, understanding, and exploiting the diversity contained within genebanks. Providing a common interface through which experimental and background data can be disseminated to both researchers and breeders will bring focus and facilitate community building into research communities. We have taken wild barley (Hordeum spp.) and potato (Solanum spp.) collections along with wheat (Triticum spp.) and maize (Zea mays subsp. mays) and their wild relatives and incorporated this data into web-based information resources built using the Germinate platform (https://ics. hutton.ac.uk/get-germinate, accessed 4 Apr. 2017). We have tailored these to better meet the demands of researchers by developing both new data visualization tools and integration with current software such as Helium, Flapjack, and CurlyWhirly (https://ics.hutton.ac.uk/software, accessed 4 Apr. 2017) and presented the data in a common platform. While the underlying species differ, the approach taken ensures that tools are compatible across all database instances. We will describe these database instances and show that Germinate offers a common platform that will aid in the exploration and wider use of these species. P.D. Shaw, S. Raubach, I. Milne, G. Stephen, and D.F. Marshall, Information and Computational Sciences, The James Hutton Institute, Errol Road, Invergowrie, Dundee, DD2 5DA, Scotland; G. Bryan and G. McKenzie, Cell and Molecular Sciences, James Hutton Institute, Errol Road, Invergowrie, Dundee, DD2 5DA, Scotland; K. Dreher and S. Hearne, International Maize and Wheat Improvement Center (CIMMYT), Texcoco, Edo. De Mexico, Mexico CP 56237. Received 28 Sept. 2016. Accepted 13 Mar. 2017. *Corresponding author (paul. shaw@hutton.ac.uk). Assigned to Associate Editor Hem Bhandari. Abbreviations: CIMMYT, Centro Internacional de Mejoramiento de Maíz y Trigo; CPC, Commonwealth Potato Collection; CWR, crop wild relative; DOI, data object identifier; GWAS, genome-wide association scan; GWT, GWT Web Toolkit; ITPGRFA, International Treaty on Plant Genetic Resources for Food and Agriculture; MCPD, MultiCrop Passport Descriptors; PCA, principal component analysis; PCO, principal coordinate analysis; SeeD, Seeds of Discovery; SNP, single nucleotide polymorphism. Published in Crop Sci. 57:1259–1273 (2017). doi: 10.2135/cropsci2016.09.0814 © Crop Science Society of America {\textbar} 5585 Guilford Rd., Madison, WI 53711 USA This is an open access article distributed under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/). Published June 16, 2017},
	author = {Shaw, P. and Raubach, S. and Hearne, S. and Dreher, K. and Bryan, G. and McKenzie, Gaynor and Milne, I. and Stephen, G. and Marshall, D.},
	year = {2017},
}

@inproceedings{zhou_katib_2019,
	title = {Katib: {A} {Distributed} {General} {AutoML} {Platform} on {Kubernetes}},
	shorttitle = {Katib},
	abstract = {Automatic Machine Learning (AutoML) is a powerful mechanism to design and tune models. We present Katib, a scalable Kubernetes-native general AutoML platform that can support a range of AutoML algorithms including both hyperparameter tuning and neural architecture search. The system is divided into separate components, encapsulated as microservices. Each micro-service operates within a Kubernetes pod and communicates with others via well-defined APIs, thus allowing flexible management and scalable deployment at a minimal cost. Together with a powerful user interface, Katib provides a universal platform for researchers as well as enterprises to try, compare and deploy their AutoML algorithms, on any Kubernetes platform.},
	booktitle = {{OpML}},
	author = {Zhou, Jinan and Velichkevich, A. and Prosvirov, Kirill and Garg, A. and Oshima, Yuji and Dutta, D.},
	year = {2019},
}

@article{yuan_bioinformatics_2020,
	title = {Bioinformatics {Application} with {Kubeflow} for {Batch} {Processing} in {Clouds}},
	doi = {10.1007/978-3-030-59851-8_24},
	abstract = {Bioinformatics pipelines make extensive use of HPC batch processing. The rapid growth of data volumes and computational complexity, especially for modern applications such as machine learning algorithms, imposes significant challenges to local HPC facilities. Many attempts have been made to burst HPC batch processing into clouds with virtual machines. They all suffer from some common issues, for example: very high overhead, slow to scale up and slow to scale down, and nearly impossible to be cloud-agnostic. We have successfully deployed and run several pipelines on Kubernetes in OpenStack, Google Cloud Platform and Amazon Web Services. In particular, we use Kubeflow on top of Kubernetes for more sophisticated job scheduling, workflow management, and first class support for machine learning. We choose Kubeflow/Kubernetes to avoid the overhead of provisioning of virtual machines, to achieve rapid scaling with containers, and to be truly cloud-agnostic in all cloud environments. Kubeflow on Kubernetes also creates some new challenges in deployment, data access, performance monitoring, etc. We will discuss the details of these challenges and provide our solutions. We will demonstrate how our solutions work across all three very different clouds for both classical pipelines and new ones for machine learning.},
	journal = {ISC Workshops},
	author = {Yuan, David and Wildish, Tony},
	year = {2020},
}

@inproceedings{simko_reana_2019,
	title = {{REANA}: {A} {System} for {Reusable} {Research} {Data} {Analyses}},
	shorttitle = {{REANA}},
	doi = {10.1051/EPJCONF/201921406034},
	abstract = {The revalidation, reinterpretation and reuse of research data analyses requires having access to the original computing environment, the experimental datasets, the analysis software, and the computational workflow steps which were used by researchers to produce the original scientific results in the first place.REANA (Reusable Analyses) is a nascent platform enabling researchers to structure their research data analyses in view of enabling future reuse. The analysis is described by means of a YAML file that captures sufficient information about the analysis assets, parameters and processes. The REANA platform consists of a set of micro-services allowing to launch and monitor container-based computational workflow jobs on the cloud. The REANA user interface and the command-line client enables researchers to easily rerun analysis workflows with new input parameters. The REANA platform aims at supporting several container technologies (Docker), workflow engines (CWL, Yadage), shared storage systems (Ceph, EOS) and compute cloud infrastructures (Ku-bernetes/OpenStack, HTCondor) used by the community.REANA was developed with the particle physics use case in mind and profits from synergies with general reusable research data analysis patterns in other scientific disciplines, such as bioinformatics and life sciences.},
	author = {Simko, T. and Heinrich, L. and Hirvonsalo, Harri and Kousidis, Dinos and Rodriguez, D.},
	year = {2019},
}

@article{banek_why_2019,
	title = {Why is the {LSST} {Science} {Platform} built on {Kubernetes}?},
	url = {http://arxiv.org/abs/1911.06404},
	abstract = {LSST has chosen Kubernetes as the platform for deploying and operating the LSST Science Platform. We first present the background reasoning behind this decision, including both instrument-agnostic as well as LSST-specific requirements. We then discuss the basic principles of Kubernetes and Helm, and how they are used as the deployment base for the LSST Science Platform. Furthermore, we provide an example of how an external group may use these publicly available software resources to deploy their own instance of the LSST Science Platform, and customize it to their needs. Finally, we discuss how more astronomy software can follow these patterns to gain similar benefits.},
	urldate = {2021-02-26},
	journal = {arXiv:1911.06404 [astro-ph]},
	author = {Banek, Christine and Thornton, Adam and Economou, Frossie and Fausti, Angelo and Krughoff, K. Simon and Sick, Jonathan},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.06404},
	keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
}

@inproceedings{shah_building_2019,
	title = {Building {Modern} {Clouds}: {Using} {Docker}, {Kubernetes} {Google} {Cloud} {Platform}},
	shorttitle = {Building {Modern} {Clouds}},
	doi = {10.1109/CCWC.2019.8666479},
	abstract = {To develop and build a modern cloud infrastructure or DevOps implementation than both Docker and Kubernetes have revolutionized the era of software development and operations. Although both are different, they unify the process of development and integration, it is now possible to build any architecture by using these technologies. Docker is used to build, ship and run any application anywhere. Docker allows the use of the same available resources. These containers can be used to make deployments much faster. Containers use less space, are reliable and are very fast. Docker Swarm helps to manage the docker container. Kubernetes is an automated container management, deployment and scaling platform. Using Google Cloud Platform to deploy containers on Kubernetes Engine enabling rapid application development and management. Kubernetes provides key features like deployment, easy ways to scale, and monitoring.},
	booktitle = {2019 {IEEE} 9th {Annual} {Computing} and {Communication} {Workshop} and {Conference} ({CCWC})},
	author = {Shah, J. and Dubaria, D.},
	month = jan,
	year = {2019},
	keywords = {Cloud, Cloud computing, Containers, DevOps implementation, Docker, Docker Swarm, Google, Google Cloud Platform, Google cloud platform, Kubernetes, Kubernetes Engine, Monitoring, Orchestration, Web servers, automated container management, cloud computing, cloud infrastructure, docker container, scaling platform, software development},
	pages = {0184--0189},
}

@article{fairbanks_ur-technical_2020,
	title = {Ur-{Technical} {Debt}},
	volume = {37},
	issn = {1937-4194},
	doi = {10.1109/MS.2020.2986613},
	abstract = {These days, everyone uses the term technical debt. It's so prevalent that we shorten it to tech debt tech debt or even just TD. Tech debt is also hacky code, code written by novices, code written without consideration of software architecture (so-called big balls of mud), and code with antipatterns flagged by static analysis tools.},
	number = {4},
	journal = {IEEE Software},
	author = {Fairbanks, G.},
	month = jul,
	year = {2020},
	note = {Conference Name: IEEE Software},
	keywords = {Codes, Computer architecture, Software engineering, hacky code, program diagnostics, source code (software), static analysis tools, tech debt, technical debt},
	pages = {95--98},
}

@misc{etheredge_software_2018,
	title = {Software {Complexity} {Is} {Killing} {Us}},
	url = {https://www.simplethread.com/software-complexity-killing-us/},
	abstract = {Since the dawn of time (before software, there was only darkness), there has been one constant: businesses want to build software cheaper and faster. It is certainly an understandable and laudable goal – especially if you’ve spent any time around software developers. It is a goal that every engineer should support wholeheartedly, and we should […]},
	language = {en-US},
	urldate = {2021-02-25},
	journal = {Simple Thread},
	author = {Etheredge, Justin},
	month = jan,
	year = {2018},
}

@article{henderson_software_2020,
	title = {Software {Engineering} at {Google}},
	url = {http://arxiv.org/abs/1702.01715},
	abstract = {We catalog and describe Google's key software engineering practices.},
	urldate = {2021-02-25},
	journal = {arXiv:1702.01715 [cs]},
	author = {Henderson, Fergus},
	month = jan,
	year = {2020},
	note = {arXiv: 1702.01715},
	keywords = {Computer Science - Software Engineering, D.2},
}

@article{csontos_accessibility_2020,
	title = {Accessibility, usability, and security evaluation of {Hungarian} government websites},
	issn = {1615-5297},
	url = {https://doi.org/10.1007/s10209-020-00716-9},
	doi = {10.1007/s10209-020-00716-9},
	abstract = {Public sector bodies are increasingly relying on the Internet. On this channel, indispensable information is transmitted to the public and a wide range of services is already available. Therefore, the usability, accessibility, and the security of these websites are very important. Accessibility is particularly crucial for persons with disabilities. The accessibility of public service websites is regulated by a number of laws; among others, the directive “on the accessibility of the websites and mobile applications of public sector bodies” adopted by the European Parliament in 2016. This obliges all European Union member states to make all public sector websites and mobile applications accessible by 23 September 2021. In practice, this means that websites must fulfil the level AA recommendations in WCAG 2.1. In our study, a website assessment method is developed by comparing different analytical tools. With this method, we analysed how Hungarian websites of public sector bodies fulfil the requirements of the directive. We have also investigated how well they comply with usability and security guidelines. The results showed that none of the 25 websites of the examined Hungarian public sector bodies could completely fulfil the recommendations of the Web Content Accessibility Guidelines (WCAG) and that half of the websites had only the lowest level of compliance in usability tests. From the security point of view, almost half of the websites use outdated server versions and programming language, which is very critical. We have proposed several suggestions to address the major problems, so website developers and administrators can improve the accessibility, usability, and security aspects of these websites.},
	language = {en},
	urldate = {2021-02-25},
	journal = {Universal Access in the Information Society},
	author = {Csontos, Balázs and Heckl, István},
	month = apr,
	year = {2020},
}

@article{shteiman_why_2014,
	title = {Why {CMS} platforms are breeding security vulnerabilities},
	volume = {2014},
	issn = {1353-4858},
	url = {https://www.sciencedirect.com/science/article/pii/S1353485814700066},
	doi = {10.1016/S1353-4858(14)70006-6},
	abstract = {Several statistics-gathering engines on the web reveal an interesting picture. Content management systems (CMS) have become far more popular in the past couple of years. A trend graph over at builtwith.com shows that over 20\% of the top 10,000 websites rely on CMS.1 And it's fair to assume that the number is higher for companies that use a CMS as a middleware between their content and their front-end website. But like all software, and this is without exception, CMSs have many security concerns. Content management systems (CMS) have become far more popular in the past couple of years. But like all software, CMSs have many security concerns. Third-party software like this is out of your control. The popularity of CMSs has been a boon for hackers, giving them a much larger surface area to attack. But, as Barry Shteiman of Imperva explains, there are steps you can take to improve your security when using CMSs.},
	language = {en},
	number = {1},
	urldate = {2021-02-25},
	journal = {Network Security},
	author = {Shteiman, Barry},
	month = jan,
	year = {2014},
	pages = {7--9},
}

@book{cern_geneva_real_2019,
	series = {{CERN} {Computing} {Seminar}},
	title = {The real costs of {Open} {Source} {Sustainability}},
	abstract = {{\textless}!--HTML--{\textgreater}{\textless}p{\textgreater}In 2016 Nadia Eghbal released "Roads and Bridges: The Unseen Labor Behind Our Digital Infrastructure," which shines a light on how few people maintain the software\&nbsp;that underpins a large amount of the internet and the services that run on it.{\textless}br /{\textgreater}{\textless}br /{\textgreater}The software world has rallied around Open Source Sustainability. Going with what they know, folks mostly focus on paying FOSS developers. Funding drives were\&nbsp;funded. Foundations were founded. Startups started up. Venture capitalists ventured that capital.{\textless}br /{\textgreater}{\textless}br /{\textgreater}Money isn't the only part of sustainable FOSS projects. Sustainability is a multi-faceted concept that can't work if people focus on only one of its many elements.{\textless}br /{\textgreater}{\textless}br /{\textgreater}This talk will:{\textless}/p{\textgreater}{\textless}ul{\textgreater}	{\textless}li{\textgreater}Review literature around the concept of sustainability{\textless}/li{\textgreater}	{\textless}li{\textgreater}Propose a definition that more accurately details what "sustainable" means to FOSS{\textless}/li{\textgreater}	{\textless}li{\textgreater}Provide tips for starting with your FOSS sustainability efforts{\textless}/li{\textgreater}{\textless}/ul{\textgreater}{\textless}p{\textgreater}{\textless}strong{\textgreater}About the speaker{\textless}/strong{\textgreater}{\textless}/p{\textgreater}{\textless}p{\textgreater}VM (aka Vicky) spent most of her twenty-plus years in the tech industry leading software development departments and teams, providing\&nbsp;technical management and leadership consulting for small and medium businesses, and helping companies understand, use, release, and\&nbsp;contribute to free and open source software in a way that's good for both their bottom line and for the community. Now, as the Director of Open\&nbsp;Source Strategy for Juniper Networks, she leverages her nearly 30 years of free and open source software experience and a strong business\&nbsp;background to help Juniper be successful through free and open source software.{\textless}br /{\textgreater}{\textless}br /{\textgreater}She is the author of\&nbsp;Forge Your Future with Open Source, the first and only book to detail how to contribute to free and open source software\&nbsp;projects. The book is published by\&nbsp;The Pragmatic Programmers\&nbsp;and is now available at\&nbsp;{\textless}a href="https://mmm.cern.ch/owa/redir.aspx?C=K1heJi2ZeKowd2cTNGHf4FN-x7kMNN0VU6RWO-9tLhSYuekX-FXXCA..\&amp;URL=https\%3a\%2f\%2ffossforge.com" target="\_blank"{\textgreater}https://fossforge.com{\textless}/a{\textgreater}.{\textless}br /{\textgreater}{\textless}br /{\textgreater}Vicky has been a moderator and author for\&nbsp;opensource.com, an author for\&nbsp;Linux Journal, the Vice President of the\&nbsp;Open Source Initiative, and is a\&nbsp;frequent and popular\&nbsp;speaker\&nbsp;at free/open source conferences and events. She's the proud winner of the Perl White Camel Award (2014), the\&nbsp;O’Reilly Open Source Award (2016), and two Opensource.com Moderator's Choice Awards (2018, 2019). She blogs about free/open source,\&nbsp;business, and technical management at\&nbsp;\{anonymous =\&gt; 'hash'\};.{\textless}/p{\textgreater}},
	language = {eng},
	editor = {CERN. Geneva and Brasseur, VM (Vicky)},
	year = {2019},
	note = {Meeting Name: The real costs of Open Source Sustainability},
	keywords = {CERN Computing Seminar, Event},
}

@misc{drupal_community_explore_nodate,
	title = {Explore featured case studies {\textbar} {Drupal}.org},
	url = {https://www.drupal.org/case-studies},
	urldate = {2021-02-21},
	author = {Drupal Community},
}

@misc{buytaert_state_nodate,
	title = {State of {Drupal} presentation ({October} 2019)},
	url = {https://dri.es/state-of-drupal-presentation-october-2019},
	abstract = {DrupalCon Amsterdam Driesnote presentation},
	language = {en},
	urldate = {2021-02-21},
	author = {Buytaert, Dries},
}

@misc{q-success_di_gelbmann_gmbh_wordpress_nodate,
	title = {{WordPress} vs. {Drupal} usage statistics, {February} 2021},
	url = {https://w3techs.com/technologies/comparison/cm-drupal,cm-wordpress},
	urldate = {2021-02-21},
	author = {Q-Success DI Gelbmann GmbH},
}

@misc{builtwith_pty_ltd_open_nodate,
	title = {Open {Source} {Usage} {Distribution} in the {Top} 10k {Sites}},
	url = {https://trends.builtwith.com/cms/open-source/traffic/Top-10k},
	urldate = {2021-02-21},
	author = {BuiltWith Pty Ltd},
}

@article{heiserman_motion-print_2020,
	title = {Motion-{Print}: {A} {Biometric} for {Real}-{Time} {Pilot} {Identification} using {Hierarchical} {Temporal} {Memory}},
	shorttitle = {Motion-{Print}},
	url = {https://www.techrxiv.org/articles/preprint/Motion-Print_A_Biometric_for_Real-Time_Pilot_Identification_using_Hierarchical_Temporal_Memory/12404393},
	doi = {10.36227/techrxiv.12404393.v1},
	abstract = {This study presents a novel biometric approach to identify operators, given only streams of their control movements within a manual control task setting. In the present task subjects control a simulated, remotely operated robotic arm, attempting to dock onto a satellite in orbit. The proposed methodology utilizes the Hierarchical Temporal Memory (HTM) algorithm to distinguish operators by their unique control behaviors. Results presented compare the identification performance of HTM with Dynamic Time Warping (DTW) and Edit Distance on Real Sequences (EDR), in both static and real-time data settings. The HTM method outperformed both DTW and EDR in the real- time setting, and matched DTW in the static setting. Observed superior performance of the HTM algorithm lays the foundation for the extension of the proposed methodology to other motion- monitoring applications, such as real-time workload assessment, motion/simulator sickness onset or distraction detection.The data gathered in the study was posted to IEEE-dataport, DOI: 10.21227/wpyf-r927},
	language = {en},
	urldate = {2020-07-15},
	author = {Heiserman, Sam and Zaychik, Kirill and Miller, Timothy},
	month = jun,
	year = {2020},
}

@article{rashkov_natural_2019,
	title = {Natural image reconstruction from brain waves: a novel visual {BCI} system with native feedback},
	copyright = {© 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {Natural image reconstruction from brain waves},
	url = {https://www.biorxiv.org/content/10.1101/787101v3},
	doi = {10.1101/787101},
	abstract = {{\textless}p{\textgreater}Here we hypothesize that observing the visual stimuli of different categories trigger distinct brain states that can be decoded from noninvasive EEG recordings. We introduce an effective closed-loop BCI system that reconstructs the observed or imagined stimuli images from the co-occurring brain wave parameters. The reconstructed images are presented to the subject as a visual feedback. The developed system is applicable to training BCI-naive subjects because of the user-friendly and intuitive way the visual patterns are employed to modify the brain states.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2019-11-08},
	journal = {bioRxiv},
	author = {Rashkov, Grigory and Bobe, Anatoly and Fastovets, Dmitry and Komarova, Maria},
	month = oct,
	year = {2019},
	pages = {787101},
}

@article{richards_deep_2019,
	title = {A deep learning framework for neuroscience},
	volume = {22},
	copyright = {2019 Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/s41593-019-0520-2},
	doi = {10.1038/s41593-019-0520-2},
	abstract = {A deep network is best understood in terms of components used to design it—objective functions, architecture and learning rules—rather than unit-by-unit computation. Richards et al. argue that this inspires fruitful approaches to systems neuroscience.},
	language = {en},
	number = {11},
	urldate = {2019-10-29},
	journal = {Nature Neuroscience},
	author = {Richards, Blake A. and Lillicrap, Timothy P. and Beaudoin, Philippe and Bengio, Yoshua and Bogacz, Rafal and Christensen, Amelia and Clopath, Claudia and Costa, Rui Ponte and Berker, Archy de and Ganguli, Surya and Gillon, Colleen J. and Hafner, Danijar and Kepecs, Adam and Kriegeskorte, Nikolaus and Latham, Peter and Lindsay, Grace W. and Miller, Kenneth D. and Naud, Richard and Pack, Christopher C. and Poirazi, Panayiota and Roelfsema, Pieter and Sacramento, João and Saxe, Andrew and Scellier, Benjamin and Schapiro, Anna C. and Senn, Walter and Wayne, Greg and Yamins, Daniel and Zenke, Friedemann and Zylberberg, Joel and Therien, Denis and Kording, Konrad P.},
	month = nov,
	year = {2019},
	pages = {1761--1770},
}

@book{sayfan_hands-microservices_2019,
	title = {Hands-{On} {Microservices} with {Kubernetes}: {Build}, deploy, and manage scalable microservices on {Kubernetes}},
	isbn = {978-1-78980-973-2},
	shorttitle = {Hands-{On} {Microservices} with {Kubernetes}},
	abstract = {Enhance your skills in building scalable infrastructure for your cloud-based applicationsKey FeaturesLearn to design a scalable architecture by building continuous integration (CI) pipelines with KubernetesGet an in-depth understanding of role-based access control (RBAC), continuous deployment (CD), and observabilityMonitor a Kubernetes cluster with Prometheus and GrafanaBook DescriptionKubernetes is among the most popular open-source platforms for automating the deployment, scaling, and operations of application containers across clusters of hosts, providing a container-centric infrastructure.Hands-On Microservices with Kubernetes starts by providing you with in-depth insights into the synergy between Kubernetes and microservices. You will learn how to use Delinkcious, which will serve as a live lab throughout the book to help you understand microservices and Kubernetes concepts in the context of a real-world application. Next, you will get up to speed with setting up a CI/CD pipeline and configuring microservices using Kubernetes ConfigMaps. As you cover later chapters, you will gain hands-on experience in securing microservices, and implementing REST, gRPC APIs, and a Delinkcious data store. In addition to this, you’ll explore the Nuclio project, run a serverless task on Kubernetes, and manage and implement data-intensive tests. Toward the concluding chapters, you’ll deploy microservices on Kubernetes and learn to maintain a well-monitored system. Finally, you’ll discover the importance of service meshes and how to incorporate Istio into the Delinkcious cluster. By the end of this book, you’ll have gained the skills you need to implement microservices on Kubernetes with the help of effective tools and best practices.What you will learnUnderstand the synergy between Kubernetes and microservicesCreate a complete CI/CD pipeline for your microservices on KubernetesDevelop microservices on Kubernetes with the Go kit framework using best practicesManage and monitor your system using Kubernetes and open-source toolsExpose your services through REST and gRPC APIsImplement and deploy serverless functions as a serviceExternalize authentication, authorization and traffic shaping using a service meshRun a Kubernetes cluster in the cloud on Google Kubernetes EngineWho this book is forThis book is for developers, DevOps engineers, or anyone who wants to develop large-scale microservice-based systems on top of Kubernetes. If you are looking to use Kubernetes on live production projects or want to migrate existing systems to a modern containerized microservices system, then this book is for you. Coding skills, together with some knowledge of Docker, Kubernetes, and cloud concepts will be useful.},
	language = {en},
	publisher = {Packt Publishing Ltd},
	author = {Sayfan, Gigi},
	month = jul,
	year = {2019},
	note = {Google-Books-ID: AEahDwAAQBAJ},
	keywords = {Computers / Hardware / Peripherals, Computers / Networking / General, Computers / Security / Networking, Computers / System Administration / Storage \& Retrieval, Computers / Systems Architecture / Distributed Systems \& Computing},
}

@article{fan_brief_2019,
	title = {A {Brief} {History} of {Simulation} {Neuroscience}},
	volume = {13},
	issn = {1662-5196},
	url = {https://www.frontiersin.org/articles/10.3389/fninf.2019.00032/full},
	doi = {10.3389/fninf.2019.00032},
	abstract = {Our knowledge of the brain has evolved over millennia in philosophical, experimental and theoretical phases. We suggest that the next phase is simulation neuroscience. The main drivers of simulation neuroscience are big data generated at multiple levels of brain organization and the need to integrate these data to trace the causal chain of interactions within and across all these levels. Simulation neuroscience is currently the only methodology for systematically approaching the multiscale brain. In this review, we attempt to reconstruct the deep historical paths leading to simulation neuroscience, from the first observations of the nerve cell to modern efforts to digitally reconstruct and simulate the brain. Neuroscience began with the identification of the neuron as the fundamental unit of brain structure and function and has evolved towards understanding the role of each cell type in the brain, how brain cells are connected to each other, and how the seemingly infinite networks they form give rise to the vast diversity of brain functions. Neuronal mapping is evolving from subjective descriptions of cell types towards objective classes, subclasses and types. Connectivity mapping is evolving from loose topographic maps between brain regions towards dense anatomical and physiological maps of connections between individual genetically distinct neurons. Functional mapping is evolving from psychological and behavioral stereotypes towards a map of behaviors emerging from structural and functional connectomes. We show how industrialization of neuroscience and the resulting large disconnected datasets are generating demand for integrative neuroscience, how the scale of neuronal and connectivity maps is driving digital atlasing and digital reconstruction to piece together the multiple levels of brain organization, and how the complexity of the interactions between molecules, neurons, microcircuits and brain regions is driving brain simulation to understand the interactions in the multiscale brain.},
	language = {English},
	urldate = {2019-08-29},
	journal = {Frontiers in Neuroinformatics},
	author = {Fan, Xue and Markram, Henry},
	year = {2019},
	keywords = {Brain structure and function, Digital reconstruction, History, connectome, neuronal types, simulation neuroscience},
}

@misc{pronschinske_47_nodate,
	title = {47 advanced tutorials for mastering {Kubernetes}},
	url = {https://techbeacon.com/enterprise-it/47-advanced-tutorials-mastering-kubernetes},
	abstract = {If you're serious about learning Kubernetes, or want to grow your knowledge about the platform in a specific area, look here first.},
	language = {en},
	urldate = {2019-08-28},
	journal = {TechBeacon},
	author = {Pronschinske, Mitch},
}

@misc{containerum_how_2018,
	title = {How to easily deploy a {Drupal} instance on {Kubernetes}},
	url = {https://medium.com/containerum/how-to-easily-deploy-a-drupal-8-instance-on-kubernetes-b90acc7786b7},
	abstract = {We have updated the tutorial to make it easier to deploy Drupal. We will be deploying Drupal 8 with MySQL on Kubernetes.},
	language = {en},
	urldate = {2019-08-28},
	journal = {Medium},
	author = {Containerum},
	month = dec,
	year = {2018},
}

@misc{iannario_getting_2019,
	title = {Getting started with {Drupal} 8 on {Kubernetes}},
	url = {https://medium.com/faun/getting-started-with-drupal-8-on-kubernetes-4d9086473848},
	abstract = {This is tutorial to deploy a new Drupal Website on a Kubernetes cluster hosted on Oracle Container Engine (OKE)},
	language = {en},
	urldate = {2019-08-28},
	journal = {Medium},
	author = {Iannario, Luca},
	month = jul,
	year = {2019},
}

@misc{noauthor_running_nodate,
	title = {Running {Drupal} in {Kubernetes} with {Docker} in production {\textbar} {Jeff} {Geerling}},
	url = {https://www.jeffgeerling.com/blog/2019/running-drupal-kubernetes-docker-production},
	urldate = {2019-08-28},
}

@misc{noauthor_drupal_nodate,
	title = {A {Drupal} {Operator} for {Kubernetes} with the {Ansible} {Operator} {SDK} {\textbar} {Jeff} {Geerling}},
	url = {https://www.jeffgeerling.com/blog/2019/drupal-operator-kubernetes-ansible-operator-sdk},
	urldate = {2019-08-28},
}

@misc{juggery_closer_2019,
	title = {A {Closer} {Look} at {Etcd}: {The} {Brain} of a {Kubernetes} {Cluster}},
	shorttitle = {A {Closer} {Look} at {Etcd}},
	url = {https://medium.com/better-programming/a-closer-look-at-etcd-the-brain-of-a-kubernetes-cluster-788c8ea759a5},
	abstract = {What etcd contains and how it organizes information},
	language = {en},
	urldate = {2019-08-28},
	journal = {Medium},
	author = {Juggery, Luc},
	month = aug,
	year = {2019},
}

@article{einevoll_scientific_2019,
	title = {The {Scientific} {Case} for {Brain} {Simulations}},
	volume = {102},
	issn = {0896-6273},
	url = {http://www.sciencedirect.com/science/article/pii/S0896627319302909},
	doi = {10.1016/j.neuron.2019.03.027},
	abstract = {A key element of the European Union’s Human Brain Project (HBP) and other large-scale brain research projects is the simulation of large-scale model networks of neurons. Here, we argue why such simulations will likely be indispensable for bridging the scales between the neuron and system levels in the brain, and why a set of brain simulators based on neuron models at different levels of biological detail should therefore be developed. To allow for systematic refinement of candidate network models by comparison with experiments, the simulations should be multimodal in the sense that they should predict not only action potentials, but also electric, magnetic, and optical signals measured at the population and system levels.},
	number = {4},
	urldate = {2019-08-26},
	journal = {Neuron},
	author = {Einevoll, Gaute T. and Destexhe, Alain and Diesmann, Markus and Grün, Sonja and Jirsa, Viktor and de Kamps, Marc and Migliore, Michele and Ness, Torbjørn V. and Plesser, Hans E. and Schürmann, Felix},
	month = may,
	year = {2019},
	keywords = {brain simulation, model, network, neuron, simulation, simulator},
	pages = {735--744},
}

@misc{noauthor_compute_nodate,
	title = {Compute the intersection of two regular expressions in {Python} 3 @ {Things} {Of} {Interest}},
	url = {https://qntm.org/greenery},
	urldate = {2019-07-23},
}

@misc{qntm_fsm/regex_2019,
	title = {{FSM}/regex conversion library. {Contribute} to qntm/greenery development by creating an account on {GitHub}},
	copyright = {MIT},
	url = {https://github.com/qntm/greenery},
	urldate = {2019-07-23},
	author = {qntm},
	month = jul,
	year = {2019},
	note = {original-date: 2012-07-22T17:10:11Z},
}

@misc{noauthor_qntm/greenery:_nodate,
	title = {qntm/greenery: {FSM}/regex conversion library},
	url = {https://github.com/qntm/greenery},
	urldate = {2019-07-23},
}

@article{mohammadi_automated_2017,
	title = {Automated {Design} of {Synthetic} {Cell} {Classifier} {Circuits} {Using} a {Two}-{Step} {Optimization} {Strategy}},
	volume = {4},
	issn = {2405-4712},
	url = {https://www.cell.com/cell-systems/abstract/S2405-4712(17)30003-0},
	doi = {10.1016/j.cels.2017.01.003},
	abstract = {{\textless}h2{\textgreater}Summary{\textless}/h2{\textgreater}{\textless}p{\textgreater}Cell classifiers are genetic logic circuits that transduce endogenous molecular inputs into cell-type-specific responses. Designing classifiers that achieve optimal differential response between specific cell types is a hard computational problem because it involves selection of endogenous inputs and optimization of both biochemical parameters and a logic function. To address this problem, we first derive an optimal set of biochemical parameters with the largest expected differential response over a diverse set of logic circuits, and second, we use these parameters in an evolutionary algorithm to select circuit inputs and optimize the logic function. Using this approach, we design experimentally feasible microRNA-based circuits capable of perfect discrimination for several real-world cell-classification tasks. We also find that under realistic cell-to-cell variation, circuit performance is comparable to standard cross-validation performance estimates. Our approach facilitates the generation of candidate circuits for experimental testing in therapeutic settings that require precise cell targeting, such as cancer therapy.{\textless}/p{\textgreater}},
	language = {English},
	number = {2},
	urldate = {2019-07-17},
	journal = {Cell Systems},
	author = {Mohammadi, Pejman and Beerenwinkel, Niko and Benenson, Yaakov},
	month = feb,
	year = {2017},
	pmid = {28189580},
	pages = {207--218.e14},
}

@article{xie_multi-input_2011,
	title = {Multi-input {RNAi}-based logic circuit for identification of specific cancer cells},
	volume = {333},
	issn = {1095-9203},
	doi = {10.1126/science.1205527},
	abstract = {Engineered biological systems that integrate multi-input sensing, sophisticated information processing, and precisely regulated actuation in living cells could be useful in a variety of applications. For example, anticancer therapies could be engineered to detect and respond to complex cellular conditions in individual cells with high specificity. Here, we show a scalable transcriptional/posttranscriptional synthetic regulatory circuit--a cell-type "classifier"--that senses expression levels of a customizable set of endogenous microRNAs and triggers a cellular response only if the expression levels match a predetermined profile of interest. We demonstrate that a HeLa cancer cell classifier selectively identifies HeLa cells and triggers apoptosis without affecting non-HeLa cell types. This approach also provides a general platform for programmed responses to other complex cell states.},
	language = {eng},
	number = {6047},
	journal = {Science (New York, N.Y.)},
	author = {Xie, Zhen and Wroblewska, Liliana and Prochazka, Laura and Weiss, Ron and Benenson, Yaakov},
	month = sep,
	year = {2011},
	pmid = {21885784},
	keywords = {Apoptosis, Biomarkers, Tumor, Cell Line, Gene Expression Regulation, Neoplastic, Gene Regulatory Networks, HeLa Cells, Humans, MicroRNAs, RNA Interference, Synthetic Biology, Transfection, bcl-2-Associated X Protein},
	pages = {1307--1311},
}

@article{wang_microrna_2016,
	title = {{MicroRNA} as {Biomarkers} and {Diagnostics}},
	volume = {231},
	copyright = {© 2015 Wiley Periodicals, Inc.},
	issn = {1097-4652},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jcp.25056},
	doi = {10.1002/jcp.25056},
	abstract = {MicroRNAs (miRNAs) are a group of small non-coding RNAs that are involved in regulating a range of developmental and physiological processes; their dysregulation has been associated with development of diseases including cancer. Circulating miRNAs and exosomal miRNAs have also been proposed as being useful in diagnostics as biomarkers for diseases and different types of cancer. In this review, miRNAs are discussed as biomarkers for cancer and other diseases, including viral infections, nervous system disorders, cardiovascular disorders, and diabetes. We summarize some of the clinical evidence for the use of miRNAs as biomarkers in diagnostics and provide some general perspectives on their use in clinical situations. The analytical challenges in using miRNAs in cancer and disease diagnostics are evaluated and discussed. Validation of specific miRNA signatures as biomarkers is a critical milestone in diagnostics. J. Cell. Physiol. 230: 25–30, 2016. © 2015 Wiley Periodicals, Inc.},
	language = {en},
	number = {1},
	urldate = {2019-07-17},
	journal = {Journal of Cellular Physiology},
	author = {Wang, Jin and Chen, Jinyun and Sen, Subrata},
	year = {2016},
	pages = {25--30},
}

@article{gao_microrna_2016,
	title = {{MicroRNA} ({miRNA}) {Profiling}},
	volume = {1381},
	issn = {1940-6029},
	doi = {10.1007/978-1-4939-3204-7_8},
	abstract = {MicroRNAs (miRNAs) are small, highly conserved noncoding RNA molecules involved in the regulation of gene expression. Since each miRNA regulates the expression of hundreds of target mRNAs, miRNAs could function as master coordinators, efficiently regulating fundamental cellular processes, including proliferation, apoptosis, and development. Furthermore, miRNAs may provide useful diagnostic and therapeutic targets in a variety of diseases. However, miRNA expression profiling is essential for the investigation of the biological functions and clinical applications of miRNAs. Therefore, in this chapter, we review and discuss commonly used techniques for miRNAs profiling, as well as their advantages and restrictions.},
	language = {eng},
	journal = {Methods in Molecular Biology (Clifton, N.J.)},
	author = {Gao, Lu and Jiang, Feng},
	year = {2016},
	pmid = {26667459},
	keywords = {Animals, DDPCR, Gene Expression Profiling, Gene Library, High-Throughput Nucleotide Sequencing, Humans, MicroRNAs, Microarray, Oligonucleotide Array Sequence Analysis, Polymerase Chain Reaction, miRNA},
	pages = {151--161},
}

@article{pritchard_microrna_2012,
	title = {{MicroRNA} profiling: approaches and considerations},
	volume = {13},
	issn = {1471-0056},
	shorttitle = {{MicroRNA} profiling},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4517822/},
	doi = {10.1038/nrg3198},
	abstract = {MicroRNAs (miRNAs) are small RNAs ({\textasciitilde}22 nt long) that post-transcriptionally regulate the expression of thousands of genes in a broad range of organisms, in both normal physiologic and disease contexts. MiRNA expression profiling is gaining popularity because miRNAs, as key regulators in gene expression networks, can influence many biological processes and have also shown promise as biomarkers for disease. Technological advances have enabled the development of various platforms for miRNA profiling, and an understanding of the strengths and pitfalls of different approaches can aid in the effective use of miRNA profiling for diverse applications. We review here the major considerations for carrying out and interpreting results of miRNA profiling studies, as well as current and emerging applications of miRNA profiling.},
	number = {5},
	urldate = {2019-07-08},
	journal = {Nature reviews. Genetics},
	author = {Pritchard, Colin C. and Cheng, Heather H. and Tewari, Muneesh},
	month = apr,
	year = {2012},
	pmid = {22510765},
	pmcid = {PMC4517822},
	pages = {358--369},
}

@article{foldiak_forming_1990,
	title = {Forming sparse representations by local anti-{Hebbian} learning},
	volume = {64},
	issn = {1432-0770},
	url = {https://doi.org/10.1007/BF02331346},
	doi = {10.1007/BF02331346},
	abstract = {How does the brain form a useful representation of its environment? It is shown here that a layer of simple Hebbian units connected by modifiable anti-Hebbian feed-back connections can learn to code a set of patterns in such a way that statistical dependency between the elements of the representation is reduced, while information is preserved. The resulting code is sparse, which is favourable if it is to be used as input to a subsequent supervised associative layer. The operation of the network is demonstrated on two simple problems.},
	language = {en},
	number = {2},
	urldate = {2019-06-24},
	journal = {Biological Cybernetics},
	author = {Földiák, P.},
	month = dec,
	year = {1990},
	keywords = {Associative Layer, Simple Problem, Sparse Representation, Statistical Dependency},
	pages = {165--170},
}

@article{schlesiger_hippocampal_2018,
	title = {Hippocampal {Global} {Remapping} {Can} {Occur} without {Input} from the {Medial} {Entorhinal} {Cortex}},
	volume = {22},
	issn = {2211-1247},
	url = {https://www.cell.com/cell-reports/abstract/S2211-1247(18)30292-4},
	doi = {10.1016/j.celrep.2018.02.082},
	abstract = {{\textless}h2{\textgreater}Summary{\textless}/h2{\textgreater}{\textless}p{\textgreater}The high storage capacity of the episodic memory system relies on distinct representations for events that are separated in time and space. The spatial component of these computations includes the formation of independent maps by hippocampal place cells across environments, referred to as global remapping. Such remapping is thought to emerge by the switching of input patterns from specialized spatially selective cells in medial entorhinal cortex (mEC), such as grid and border cells. Although it has been shown that acute manipulations of mEC firing patterns are sufficient for inducing hippocampal remapping, it remains unknown whether specialized spatial mEC inputs are necessary for the reorganization of hippocampal spatial representations. Here, we examined remapping in rats without mEC input to the hippocampus and found that highly distinct spatial maps emerged rapidly in every individual rat. Our data suggest that hippocampal spatial computations do not depend on inputs from specialized cell types in mEC.{\textless}/p{\textgreater}},
	language = {English},
	number = {12},
	urldate = {2019-06-21},
	journal = {Cell Reports},
	author = {Schlesiger, Magdalene I. and Boublil, Brittney L. and Hales, Jena B. and Leutgeb, Jill K. and Leutgeb, Stefan},
	month = mar,
	year = {2018},
	pmid = {29562172},
	pages = {3152--3159},
}

@misc{noauthor_hippocampal_nodate,
	title = {Hippocampal {Global} {Remapping} {Can} {Occur} without {Input} from the {Medial} {Entorhinal} {Cortex}: {Cell} {Reports}},
	url = {https://www.cell.com/cell-reports/fulltext/S2211-1247(18)30292-4},
	urldate = {2019-06-21},
}

@article{chazelle_natural_2012,
	title = {Natural algorithms and influence systems},
	volume = {55},
	issn = {0001-0782},
	url = {http://dl.acm.org/citation.cfm?id=2380656.2380679},
	doi = {10.1145/2380656.2380679},
	number = {12},
	urldate = {2019-06-20},
	journal = {Communications of the ACM},
	author = {Chazelle, Bernard},
	month = dec,
	year = {2012},
	pages = {101--110},
}

@misc{chazelle_natural_nodate,
	title = {Natural {Algorithms} and {Influence} {Systems}},
	url = {https://cacm.acm.org/magazines/2012/12/157889-natural-algorithms-and-influence-systems/abstract},
	abstract = {Algorithms lay the grounds for numerical simulations and, crucially, provide a powerful framework for their analysis. The new area of natural algorithms may reprise in the life sciences the role differential equations have long played in the physical sciences.},
	language = {en},
	urldate = {2019-06-20},
	author = {Chazelle, Bernard},
}

@article{kipouridis_convergence_2019,
	title = {On the {Convergence} of {Network} {Systems}},
	url = {http://arxiv.org/abs/1902.04121},
	abstract = {The apparent disconnection between the microscopic and the macroscopic is a major issue in the understanding of complex systems. To this extend, we study the convergence of repeatedly applying local rules on a network, and touch on the expressive power of this model. We look at network systems and study their behavior when different types of local rules are applied on them. For a very general class of local rules, we prove convergence and provide a certain member of this class that, when applied on a graph, efficiently computes its k-core and its (k-1)-crust giving hints on the expressive power of such a model. Furthermore, we provide guarantees on the speed of convergence for an important subclass of the aforementioned class. We also study more general rules, and show that they do not converge. Our counterexamples resolve an open question of (Zhang, Wang, Wang, Zhou, KDD- 2009) as well, concerning whether a certain process converges. Finally, we show the universality of our network system, by providing a local rule under which it is Turing-Complete.},
	urldate = {2019-06-10},
	journal = {arXiv:1902.04121 [cs, math]},
	author = {Kipouridis, Evangelos and Tsichlas, Kostas},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.04121},
	keywords = {Computer Science - Data Structures and Algorithms, F.1.1, F.2.2, G.2.2, Mathematics - Dynamical Systems},
}

@article{hawkins_framework_2019,
	title = {A {Framework} for {Intelligence} and {Cortical} {Function} {Based} on {Grid} {Cells} in the {Neocortex}},
	volume = {12},
	issn = {1662-5110},
	url = {https://www.frontiersin.org/articles/10.3389/fncir.2018.00121/full},
	doi = {10.3389/fncir.2018.00121},
	abstract = {How the neocortex works is a mystery. In this paper we propose a novel framework for understanding its function. Grid cells are neurons in the entorhinal cortex that represent the location of an animal in its environment. Recent evidence suggests that grid cell-like neurons may also be present in the neocortex. We propose that grid cells exist throughout the neocortex, in every region and in every cortical column. They define a location-based framework for how the neocortex functions. Whereas grid cells in the entorhinal cortex represent the location of one thing, the body relative to its environment, we propose that cortical grid cells simultaneously represent the location of many things. Cortical columns in somatosensory cortex track the location of tactile features relative to the object being touched and cortical columns in visual cortex track the location of visual features relative to the object being viewed. We propose that mechanisms in the entorhinal cortex and hippocampus that evolved for learning the structure of environments are now used by the neocortex to learn the structure of objects. Having a representation of location in each cortical column suggests mechanisms for how the neocortex represents object compositionality and object behaviors. It leads to the hypothesis that every part of the neocortex learns complete models of objects and that there are many models of each object distributed throughout the neocortex. The similarity of circuitry observed in all cortical regions is strong evidence that even high-level cognitive tasks are learned and represented in a location-based framework.},
	language = {English},
	urldate = {2019-06-10},
	journal = {Frontiers in Neural Circuits},
	author = {Hawkins, Jeff and Lewis, Marcus and Klukas, Mirko and Purdy, Scott and Ahmad, Subutai},
	year = {2019},
	keywords = {Grid Cell, Neocortex, cortical column, hierarchy, neocortical theory, object recognition},
}

@inproceedings{shcherbakov_survey_2013,
	title = {A {Survey} of {Forecast} {Error} {Measures}},
	abstract = {Submitted: Aug 7, 2013; Accepted: Sep 18, 2013; Published: Sep 25, 2013 Abstract: This article reviews the common used forecast error measurements. All error measurements have been joined in the seven groups: absolute forecasting errors, measures based on percentage errors, symmetric errors, measures based on relative errors, scaled errors, relative measures and other error measures. The formulas are presented and drawbacks are discussed for every accuracy measurements. To reduce the impact of outliers, an Integral Normalized Mean Square Error have been proposed. Due to the fact that each error measure has the disadvantages that can lead to inaccurate evaluation of the forecasting results, it is impossible to choose only one measure, the recommendations for selecting the appropriate error measurements are given.},
	author = {Shcherbakov, Maxim and Brebels, Adriaan and Tyukov, Anton and Janovsky, Timur and Anatol, Valeriy},
	year = {2013},
	keywords = {Mean squared error, Projections and Predictions},
}

@misc{noauthor_survey_nodate,
	title = {A survey of forecast error measures},
	url = {https://www.researchgate.net/publication/281718517_A_survey_of_forecast_error_measures},
	abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
	language = {en},
	urldate = {2019-06-10},
	journal = {ResearchGate},
}

@misc{numenta_nupic_2019,
	title = {{NuPIC} - {HotGym} {Prediction}},
	shorttitle = {Numenta {Platform} for {Intelligent} {Computing} is an implementation of {Hierarchical} {Temporal} {Memory} ({HTM}), a theory of intelligence based strictly on the neuroscience of the neocortex.},
	url = {https://github.com/numenta/nupic/tree/master/examples/opf/clients/hotgym/prediction/one_gym},
	urldate = {2019-06-10},
	publisher = {Numenta},
	author = {Numenta},
	month = jun,
	year = {2019},
	note = {original-date: 2013-04-05T23:14:27Z},
}

@misc{numenta_nupic_nodate,
	title = {{NUPIC}},
	url = {https://github.com/numenta/nupic},
	urldate = {2019-06-04},
	author = {Numenta},
}

@misc{innes_lazy.jl_nodate,
	title = {Lazy.jl},
	url = {https://github.com/MikeInnes/Lazy.jl},
	urldate = {2019-06-04},
	author = {Innes, Mike},
}

@misc{aglassinger_pygount_2019,
	title = {pygount},
	copyright = {BSD-3-Clause},
	url = {https://github.com/roskakori/pygount},
	urldate = {2019-06-10},
	author = {Aglassinger, Thomas},
	month = may,
	year = {2019},
	note = {original-date: 2013-04-13T13:02:29Z},
}

@book{stallman_free_2002,
	title = {Free {Software}, {Free} {Society}: {Selected} {Essays} of {Richard} {M}. {Stallman}},
	isbn = {978-1-882114-98-6},
	shorttitle = {Free {Software}, {Free} {Society}},
	abstract = {Essay Collection covering the point where software, law and social justice meet.},
	language = {en},
	publisher = {Lulu.com},
	author = {Stallman, Richard},
	year = {2002},
	note = {Google-Books-ID: UJlNAgAAQBAJ},
	keywords = {Computers / Social Aspects / General, Law / Intellectual Property / General, Philosophy / Ethics \& Moral Philosophy},
}

@article{knuth_literate_1984,
	title = {Literate {Programming}},
	volume = {27},
	issn = {0010-4620},
	url = {https://academic.oup.com/comjnl/article/27/2/97/343244},
	doi = {10.1093/comjnl/27.2.97},
	abstract = {Abstract.  The author and his associates have been experimenting for the past several years with a programming language and documentation system called WEB. Thi},
	language = {en},
	number = {2},
	urldate = {2019-06-09},
	journal = {The Computer Journal},
	author = {Knuth, D. E.},
	month = jan,
	year = {1984},
	pages = {97--111},
}

@misc{pastell_weave.jl:_2017,
	title = {Weave.jl: {Scientific} {Reports} {Using} {Julia}},
	shorttitle = {Weave.jl},
	url = {http://joss.theoj.org},
	abstract = {Pastell, (2017), Weave.jl: Scientific Reports Using Julia, Journal of Open Source Software, 2(11), 204, doi:10.21105/joss.00204},
	language = {en},
	urldate = {2019-06-09},
	journal = {The Journal of Open Source Software},
	author = {Pastell, Matti},
	month = mar,
	year = {2017},
	doi = {10.21105/joss.00204},
}

@inproceedings{shah_novel_2013,
	title = {Novel algebras for advanced analytics in {Julia}},
	doi = {10.1109/HPEC.2013.6670347},
	abstract = {A linear algebraic approach to graph algorithms that exploits the sparse adjacency matrix representation of graphs can provide a variety of benefits. These benefits include syntactic simplicity, easier implementation, and higher performance. One way to employ linear algebra techniques for graph algorithms is to use a broader definition of matrix and vector multiplication. We demonstrate through the use of the Julia language system how easy it is to explore semirings using linear algebraic methodologies.},
	booktitle = {2013 {IEEE} {High} {Performance} {Extreme} {Computing} {Conference} ({HPEC})},
	author = {Shah, V. B. and Edelman, A. and Karpinski, S. and Bezanson, J. and Kepner, J.},
	month = sep,
	year = {2013},
	keywords = {Electronic mail, Julia, Julia language system, Matrices, Sparse matrices, Standards, Syntactics, advanced analytics, graph algorithms, high level languages, linear algebra, linear algebra techniques, linear algebraic approach, linear algebraic methodologies, mathematics computing, matrix multiplication, novel algebras, sparse adjacency matrix representation, vector multiplication},
	pages = {1--4},
}

@misc{aglassinger_pygount_nodate,
	title = {pygount},
	url = {https://github.com/roskakori/pygount},
	urldate = {2019-06-04},
	author = {Aglassinger, Thomas},
}

@article{song_competitive_2000,
	title = {Competitive {Hebbian} learning through spike-timing-dependent synaptic plasticity},
	volume = {3},
	copyright = {2000 Nature America Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/nn0900_919},
	doi = {10.1038/78829},
	abstract = {Hebbian models of development and learning require both activity-dependent synaptic plasticity and a mechanism that induces competition between different synapses. One form of experimentally observed long-term synaptic plasticity, which we call spike-timing-dependent plasticity (STDP), depends on the relative timing of pre- and postsynaptic action potentials. In modeling studies, we find that this form of synaptic modification can automatically balance synaptic strengths to make postsynaptic firing irregular but more sensitive to presynaptic spike timing. It has been argued that neurons in vivo operate in such a balanced regime. Synapses modifiable by STDP compete for control of the timing of postsynaptic action potentials. Inputs that fire the postsynaptic neuron with short latency or that act in correlated groups are able to compete most successfully and develop strong synapses, while synapses of longer-latency or less-effective inputs are weakened.},
	language = {En},
	number = {9},
	urldate = {2019-06-02},
	journal = {Nature Neuroscience},
	author = {Song, Sen and Miller, Kenneth D. and Abbott, L. F.},
	month = sep,
	year = {2000},
	pages = {919},
}

@article{rossum_stable_2000,
	title = {Stable {Hebbian} {Learning} from {Spike} {Timing}-{Dependent} {Plasticity}},
	volume = {20},
	copyright = {Copyright © 2000 Society for Neuroscience},
	issn = {0270-6474, 1529-2401},
	url = {http://www.jneurosci.org/content/20/23/8812},
	doi = {10.1523/JNEUROSCI.20-23-08812.2000},
	abstract = {We explore a synaptic plasticity model that incorporates recent findings that potentiation and depression can be induced by precisely timed pairs of synaptic events and postsynaptic spikes. In addition we include the observation that strong synapses undergo relatively less potentiation than weak synapses, whereas depression is independent of synaptic strength. After random stimulation, the synaptic weights reach an equilibrium distribution which is stable, unimodal, and has positive skew. This weight distribution compares favorably to the distributions of quantal amplitudes and of receptor number observed experimentally in central neurons and contrasts to the distribution found in plasticity models without size-dependent potentiation. Also in contrast to those models, which show strong competition between the synapses, stable plasticity is achieved with little competition. Instead, competition can be introduced by including a separate mechanism that scales synaptic strengths multiplicatively as a function of postsynaptic activity. In this model, synaptic weights change in proportion to how correlated they are with other inputs onto the same postsynaptic neuron. These results indicate that stable correlation-based plasticity can be achieved without introducing competition, suggesting that plasticity and competition need not coexist in all circuits or at all developmental stages.},
	language = {en},
	number = {23},
	urldate = {2019-06-02},
	journal = {Journal of Neuroscience},
	author = {Rossum, M. C. W. van and Bi, G. Q. and Turrigiano, G. G.},
	month = dec,
	year = {2000},
	pmid = {11102489},
	keywords = {Hebbian plasticity, activity-dependent scaling, stochastic approaches, synaptic competition, synaptic weights, temporal learning},
	pages = {8812--8821},
}

@article{foldiak_forming_1990,
	title = {Forming sparse representations by local anti-{Hebbian} learning},
	volume = {64},
	issn = {1432-0770},
	url = {https://doi.org/10.1007/BF02331346},
	doi = {10.1007/BF02331346},
	abstract = {How does the brain form a useful representation of its environment? It is shown here that a layer of simple Hebbian units connected by modifiable anti-Hebbian feed-back connections can learn to code a set of patterns in such a way that statistical dependency between the elements of the representation is reduced, while information is preserved. The resulting code is sparse, which is favourable if it is to be used as input to a subsequent supervised associative layer. The operation of the network is demonstrated on two simple problems.},
	language = {en},
	number = {2},
	urldate = {2019-06-02},
	journal = {Biological Cybernetics},
	author = {Földiák, P.},
	month = dec,
	year = {1990},
	keywords = {Associative Layer, Simple Problem, Sparse Representation, Statistical Dependency},
	pages = {165--170},
}

@article{cui_htm_2017,
	title = {The {HTM} {Spatial} {Pooler}—{A} {Neocortical} {Algorithm} for {Online} {Sparse} {Distributed} {Coding}},
	volume = {11},
	issn = {1662-5188},
	url = {https://www.frontiersin.org/articles/10.3389/fncom.2017.00111/full},
	doi = {10.3389/fncom.2017.00111},
	abstract = {Hierarchical temporal memory (HTM) provides a theoretical framework that models several key computational principles of the neocortex. In this paper we analyze an important component of HTM, the HTM spatial pooler (SP). The SP models how neurons learn feedforward connections and form efficient representations of the input. It converts arbitrary binary input patterns into sparse distributed representations (SDRs) using a combination of competitive Hebbian learning rules and homeostatic excitability control. We describe a number of key properties of the spatial pooler, including fast adaptation to changing input statistics, improved noise robustness through learning, efficient use of cells and robustness to cell death. In order to quantify these properties we develop a set of metrics that can be directly computed from the spatial pooler outputs. We show how the properties are met using these metrics and targeted artificial simulations. We then demonstrate the value of the spatial pooler in a complete end-to-end real-world HTM system. We discuss the relationship with neuroscience and previous studies of sparse coding. The HTM spatial pooler represents a neurally inspired algorithm for learning sparse representations from noisy data streams in an online fashion.},
	language = {English},
	urldate = {2019-06-02},
	journal = {Frontiers in Computational Neuroscience},
	author = {Cui, Yuwei and Ahmad, Subutai and Hawkins, Jeff},
	year = {2017},
	keywords = {Hebbian Learning, Hierarchical temporal memory (HTM), Online Learning, competitive learning, sparse distributed representations, sparse representation, spatial pooling},
}

@article{purdy_encoding_2016,
	title = {Encoding {Data} for {HTM} {Systems}},
	url = {http://arxiv.org/abs/1602.05925},
	abstract = {Hierarchical Temporal Memory (HTM) is a biologically inspired machine intelligence technology that mimics the architecture and processes of the neocortex. In this white paper we describe how to encode data as Sparse Distributed Representations (SDRs) for use in HTM systems. We explain several existing encoders, which are available through the open source project called NuPIC, and we discuss requirements for creating encoders for new types of data.},
	urldate = {2019-06-02},
	journal = {arXiv:1602.05925 [cs, q-bio]},
	author = {Purdy, Scott},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.05925},
	keywords = {Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
}

@inproceedings{meier_mixed-signal_2015,
	title = {A mixed-signal universal neuromorphic computing system},
	doi = {10.1109/IEDM.2015.7409627},
	abstract = {Neuromorphic information processing systems offer the potential to overcome imminent problems of state-of-the-art computers, in particular the energy efficiency problem, the device reliability problem and the software complexity problem. This paper starts with a short overview of state-of-the-art neuromorphic hardware implementations and their applications. It then describes the time-accelerated mixed-signal approach of the BrainScaleS project in some detail.},
	booktitle = {2015 {IEEE} {International} {Electron} {Devices} {Meeting} ({IEDM})},
	author = {Meier, K.},
	month = dec,
	year = {2015},
	keywords = {Biological system modeling, Brain modeling, BrainScaleS project, Computational modeling, Energy efficiency, Neuromorphics, Neurons, VLSI, device reliability problem, energy efficiency problem, mixed-signal universal neuromorphic computing system, neural net architecture, neuromorphic hardware implementations, neuromorphic information processing systems, power aware computing, software complexity problem, software metrics, time-accelerated mixed-signal approach},
	pages = {4.6.1--4.6.4},
}

@article{cui_continuous_2016,
	title = {Continuous {Online} {Sequence} {Learning} with an {Unsupervised} {Neural} {Network} {Model}},
	volume = {28},
	issn = {0899-7667},
	url = {https://www.mitpressjournals.org/doi/full/10.1162/NECO_a_00893},
	doi = {10.1162/NECO_a_00893},
	abstract = {The ability to recognize and predict temporal sequences of sensory inputs is vital for survival in natural environments. Based on many known properties of cortical neurons, hierarchical temporal memory (HTM) sequence memory recently has been proposed as a theoretical framework for sequence learning in the cortex. In this letter, we analyze properties of HTM sequence memory and apply it to sequence learning and prediction problems with streaming data. We show the model is able to continuously learn a large number of variable order temporal sequences using an unsupervised Hebbian-like learning rule. The sparse temporal codes formed by the model can robustly handle branching temporal sequences by maintaining multiple predictions until there is sufficient disambiguating evidence. We compare the HTM sequence memory with other sequence learning algorithms, including statistical methods—autoregressive integrated moving average; feedforward neural networks—time delay neural network and online sequential extreme learning machine; and recurrent neural networks—long short-term memory and echo-state networks on sequence prediction problems with both artificial and real-world data. The HTM model achieves comparable accuracy to other state-of-the-art algorithms. The model also exhibits properties that are critical for sequence learning, including continuous online learning, the ability to handle multiple predictions and branching sequences with high-order statistics, robustness to sensor noise and fault tolerance, and good performance without task-specific hyperparameter tuning. Therefore, the HTM sequence memory not only advances our understanding of how the brain may solve the sequence learning problem but is also applicable to real-world sequence learning problems from continuous data streams.},
	number = {11},
	urldate = {2019-06-02},
	journal = {Neural Computation},
	author = {Cui, Yuwei and Ahmad, Subutai and Hawkins, Jeff},
	month = sep,
	year = {2016},
	pages = {2474--2504},
}

@article{billaudelle_porting_2015,
	title = {Porting {HTM} {Models} to the {Heidelberg} {Neuromorphic} {Computing} {Platform}},
	url = {http://arxiv.org/abs/1505.02142},
	abstract = {Hierarchical Temporal Memory (HTM) is a computational theory of machine intelligence based on a detailed study of the neocortex. The Heidelberg Neuromorphic Computing Platform, developed as part of the Human Brain Project (HBP), is a mixed-signal (analog and digital) large-scale platform for modeling networks of spiking neurons. In this paper we present the first effort in porting HTM networks to this platform. We describe a framework for simulating key HTM operations using spiking network models. We then describe specific spatial pooling and temporal memory implementations, as well as simulations demonstrating that the fundamental properties are maintained. We discuss issues in implementing the full set of plasticity rules using Spike-Timing Dependent Plasticity (STDP), and rough place and route calculations. Although further work is required, our initial studies indicate that it should be possible to run large-scale HTM networks (including plasticity rules) efficiently on the Heidelberg platform. More generally the exercise of porting high level HTM algorithms to biophysical neuron models promises to be a fruitful area of investigation for future studies.},
	urldate = {2019-06-02},
	journal = {arXiv:1505.02142 [cs, q-bio]},
	author = {Billaudelle, Sebastian and Ahmad, Subutai},
	month = may,
	year = {2015},
	note = {arXiv: 1505.02142},
	keywords = {Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
}

@book{kandel_principles_2013,
	title = {Principles of {Neural} {Science}, {Fifth} {Edition}},
	isbn = {978-0-07-139011-8},
	abstract = {Publisher's Note: Products purchased from Third Party sellers are not guaranteed by the publisher for quality, authenticity, or access to any online entitlements included with the product.Now updated: the definitive neuroscience resource—from Eric R. Kandel, MD (winner of the Nobel Prize in 2000); James H. Schwartz, MD, PhD; Thomas M. Jessell, PhD; Steven A. Siegelbaum, PhD; and A. J. Hudspeth, PhDA Doody's Core Title for 2019!900 full-color illustrationsDeciphering the link between the human brain and behavior has always been one of the most intriguing—and often challenging—aspects of scientific endeavor. The sequencing of the human genome, and advances in molecular biology, have illuminated the pathogenesis of many neurological diseases and have propelled our knowledge of how the brain controls behavior.To grasp the wider implications of these developments and gain a fundamental understanding of this dynamic, fast-moving field, Principles of Neuroscience stands alone as the most authoritative and indispensible resource of its kind.In this classic text, prominent researchers in the field expertly survey the entire spectrum of neural science, giving an up-to-date, unparalleled view of the discipline for anyone who studies brain and mind. Here, in one remarkable volume, is the current state of neural science knowledge—ranging from molecules and cells, to anatomic structures and systems, to the senses and cognitive functions—all supported by more than 900 precise, full-color illustrations. In addition to clarifying complex topics, the book also benefits from a cohesive organization, beginning with an insightful overview of the interrelationships between the brain, nervous system, genes, and behavior. Principles of Neural Science then proceeds with an in-depth examination of the molecular and cellular biology of nerve cells, synaptic transmission, and the neural basis of cognition. The remaining sections illuminate how cells, molecules, and systems give us sight, hearing, touch, movement, thought, learning, memories, and emotions.The new fifth edition of Principles of Neural Science is thoroughly updated to reflect the tremendous amount of research, and the very latest clinical perspectives, that have significantly transformed the field within the last decade.Ultimately, Principles of Neural Science affirms that all behavior is an expression of neural activity, and that the future of clinical neurology and psychiatry hinges on the progress of neural science. Far exceeding the scope and scholarship of similar texts, this unmatched guide offers a commanding, scientifically rigorous perspective on the molecular mechanisms of neural function and disease—one that you’ll continually rely on to advance your comprehension of brain, mind, and behavior.FEATURESThe cornerstone reference in the field of neuroscience that explains how the nerves, brain, and mind functionClear emphasis on how behavior can be examined through the electrical activity of both individual neurons and systems of nerve cellsCurrent focus on molecular biology as a tool for probing the pathogenesis of many neurological diseases, including muscular dystrophy, Huntington disease, and certain forms of Alzheimer’s diseaseMore than 900 engaging full-color illustrations—including line drawings, radiographs, micrographs, and medical photographs clarify often-complex neuroscience conceptsOutstanding section on the development and emergence of behavior, including important coverage of},
	language = {en},
	publisher = {McGraw Hill Professional},
	author = {Kandel, Eric R. and Jessell, Thomas M. and Schwartz, James H. and Siegelbaum, Steven A. and Hudspeth, A. J.},
	year = {2013},
	keywords = {Medical / Neurology, Medical / Neuroscience, Science / Life Sciences / Neuroscience},
}

@misc{noauthor_principles_nodate,
	title = {Principles of {Neural} {Science}, {Fifth} {Edition} {\textbar} {AccessNeurology} {\textbar} {McGraw}-{Hill} {Medical}},
	url = {https://neurology.mhmedical.com/book.aspx?bookID=1049},
	urldate = {2019-06-02},
}

@article{markram_reconstruction_2015,
	title = {Reconstruction and {Simulation} of {Neocortical} {Microcircuitry}},
	volume = {163},
	issn = {1097-4172},
	doi = {10.1016/j.cell.2015.09.029},
	abstract = {We present a first-draft digital reconstruction of the microcircuitry of somatosensory cortex of juvenile rat. The reconstruction uses cellular and synaptic organizing principles to algorithmically reconstruct detailed anatomy and physiology from sparse experimental data. An objective anatomical method defines a neocortical volume of 0.29 ± 0.01 mm(3) containing {\textasciitilde}31,000 neurons, and patch-clamp studies identify 55 layer-specific morphological and 207 morpho-electrical neuron subtypes. When digitally reconstructed neurons are positioned in the volume and synapse formation is restricted to biological bouton densities and numbers of synapses per connection, their overlapping arbors form {\textasciitilde}8 million connections with {\textasciitilde}37 million synapses. Simulations reproduce an array of in vitro and in vivo experiments without parameter tuning. Additionally, we find a spectrum of network states with a sharp transition from synchronous to asynchronous activity, modulated by physiological mechanisms. The spectrum of network states, dynamically reconfigured around this transition, supports diverse information processing strategies.
PAPERCLIP: VIDEO ABSTRACT.},
	language = {eng},
	number = {2},
	journal = {Cell},
	author = {Markram, Henry and Muller, Eilif and Ramaswamy, Srikanth and Reimann, Michael W. and Abdellah, Marwan and Sanchez, Carlos Aguado and Ailamaki, Anastasia and Alonso-Nanclares, Lidia and Antille, Nicolas and Arsever, Selim and Kahou, Guy Antoine Atenekeng and Berger, Thomas K. and Bilgili, Ahmet and Buncic, Nenad and Chalimourda, Athanassia and Chindemi, Giuseppe and Courcol, Jean-Denis and Delalondre, Fabien and Delattre, Vincent and Druckmann, Shaul and Dumusc, Raphael and Dynes, James and Eilemann, Stefan and Gal, Eyal and Gevaert, Michael Emiel and Ghobril, Jean-Pierre and Gidon, Albert and Graham, Joe W. and Gupta, Anirudh and Haenel, Valentin and Hay, Etay and Heinis, Thomas and Hernando, Juan B. and Hines, Michael and Kanari, Lida and Keller, Daniel and Kenyon, John and Khazen, Georges and Kim, Yihwa and King, James G. and Kisvarday, Zoltan and Kumbhar, Pramod and Lasserre, Sébastien and Le Bé, Jean-Vincent and Magalhães, Bruno R. C. and Merchán-Pérez, Angel and Meystre, Julie and Morrice, Benjamin Roy and Muller, Jeffrey and Muñoz-Céspedes, Alberto and Muralidhar, Shruti and Muthurasa, Keerthan and Nachbaur, Daniel and Newton, Taylor H. and Nolte, Max and Ovcharenko, Aleksandr and Palacios, Juan and Pastor, Luis and Perin, Rodrigo and Ranjan, Rajnish and Riachi, Imad and Rodríguez, José-Rodrigo and Riquelme, Juan Luis and Rössert, Christian and Sfyrakis, Konstantinos and Shi, Ying and Shillcock, Julian C. and Silberberg, Gilad and Silva, Ricardo and Tauheed, Farhan and Telefont, Martin and Toledo-Rodriguez, Maria and Tränkler, Thomas and Van Geit, Werner and Díaz, Jafet Villafranca and Walker, Richard and Wang, Yun and Zaninetta, Stefano M. and DeFelipe, Javier and Hill, Sean L. and Segev, Idan and Schürmann, Felix},
	month = oct,
	year = {2015},
	pmid = {26451489},
	keywords = {Algorithms, Animals, Computer Simulation, Hindlimb, Male, Models, Neurological, Neocortex, Nerve Net, Neurons, Rats, Rats, Wistar, Somatosensory Cortex},
	pages = {456--492},
}

@article{hawkins_theory_2017,
	title = {A {Theory} of {How} {Columns} in the {Neocortex} {Enable} {Learning} the {Structure} of the {World}},
	volume = {11},
	issn = {1662-5110},
	url = {https://www.frontiersin.org/articles/10.3389/fncir.2017.00081/full},
	doi = {10.3389/fncir.2017.00081},
	abstract = {Neocortical regions are organized into columns and layers. Connections between layers run mostly perpendicular to the surface suggesting a columnar functional organization. Some layers have long-range excitatory lateral connections suggesting interactions between columns. Similar patterns of connectivity exist in all regions but their exact role remain a mystery. In this paper, we propose a network model composed of columns and layers that performs robust object learning and recognition. Each column integrates its changing input over time to learn complete predictive models of observed objects. Excitatory lateral connections across columns allow the network to more rapidly infer objects based on the partial knowledge of adjacent columns. Because columns integrate input over time and space, the network learns models of complex objects that extend well beyond the receptive field of individual cells. Our network model introduces a new feature to cortical columns. We propose that a representation of location relative to the object being sensed is calculated within the sub-granular layers of each column. The location signal is provided as an input to the network, where it is combined with sensory data. Our model contains two layers and one or more columns. Simulations show that using Hebbian-like learning rules small single-column networks can learn to recognize hundreds of objects, with each object containing tens of features. Multi-column networks recognize objects with significantly fewer movements of the sensory receptors. Given the ubiquity of columnar and laminar connectivity patterns throughout the neocortex, we propose that columns and regions have more powerful recognition and modeling capabilities than previously assumed.},
	language = {English},
	urldate = {2019-06-01},
	journal = {Frontiers in Neural Circuits},
	author = {Hawkins, Jeff and Ahmad, Subutai and Cui, Yuwei},
	year = {2017},
	keywords = {Hierarchical temporal memory, Neocortex, cortical columns, cortical layers, sensorimotor learning},
}

@article{hawkins_why_2016,
	title = {Why {Neurons} {Have} {Thousands} of {Synapses}, a {Theory} of {Sequence} {Memory} in {Neocortex}},
	volume = {10},
	issn = {1662-5110},
	url = {https://www.frontiersin.org/articles/10.3389/fncir.2016.00023/full},
	doi = {10.3389/fncir.2016.00023},
	abstract = {Pyramidal neurons represent the majority of excitatory neurons in the neocortex. Each pyramidal neuron receives input from thousands of excitatory synapses that are segregated onto dendritic branches. The dendrites themselves are segregated into apical, basal, and proximal integration zones, which have different properties. It is a mystery how pyramidal neurons integrate the input from thousands of synapses, what role the different dendrites play in this integration, and what kind of network behavior this enables in cortical tissue. It has been previously proposed that non-linear properties of dendrites enable cortical neurons to recognize multiple independent patterns. In this paper we extend this idea in multiple ways. First we show that a neuron with several thousand synapses segregated on active dendrites can recognize hundreds of independent patterns of cellular activity even in the presence of large amounts of noise and pattern variation. We then propose a neuron model where patterns detected on proximal dendrites lead to action potentials, defining the classic receptive field of the neuron, and patterns detected on basal and apical dendrites act as predictions by slightly depolarizing the neuron without generating an action potential. By this mechanism, a neuron can predict its activation in hundreds of independent contexts. We then present a network model based on neurons with these properties that learns time-based sequences. The network relies on fast local inhibition to preferentially activate neurons that are slightly depolarized. Through simulation we show that the network scales well and operates robustly over a wide range of parameters as long as the network uses a sparse distributed code of cellular activations. We contrast the properties of the new network model with several other neural network models to illustrate the relative capabilities of each. We conclude that pyramidal neurons with thousands of synapses, active dendrites, and multiple integration zones create a robust and powerful sequence memory. Given the prevalence and similarity of excitatory neurons throughout the neocortex and the importance of sequence memory in inference and behavior, we propose that this form of sequence memory may be a universal property of neocortical tissue.},
	language = {English},
	urldate = {2019-05-31},
	journal = {Frontiers in Neural Circuits},
	author = {Hawkins, Jeff and Ahmad, Subutai},
	year = {2016},
	keywords = {Cortex, NMDA spike, Neurons, active dendrites, sparse distributed representations},
}

@article{freund_interneurons_2008,
	title = {Interneurons},
	volume = {3},
	issn = {1941-6016},
	url = {http://www.scholarpedia.org/article/Interneurons},
	doi = {10.4249/scholarpedia.4720},
	language = {en},
	number = {9},
	urldate = {2019-05-31},
	journal = {Scholarpedia},
	author = {Freund, Tamas and Kali, Szabolcs},
	month = sep,
	year = {2008},
	pages = {4720},
}

@article{mountcastle_columnar_1997,
	title = {The columnar organization of the neocortex},
	volume = {120 ( Pt 4)},
	issn = {0006-8950},
	doi = {10.1093/brain/120.4.701},
	abstract = {The modular organization of nervous systems is a widely documented principle of design for both vertebrate and invertebrate brains of which the columnar organization of the neocortex is an example. The classical cytoarchitectural areas of the neocortex are composed of smaller units, local neural circuits repeated iteratively within each area. Modules may vary in cell type and number, in internal and external connectivity, and in mode of neuronal processing between different large entities; within any single large entity they have a basic similarity of internal design and operation. Modules are most commonly grouped into entities by sets of dominating external connections. This unifying factor is most obvious for the heterotypical sensory and motor areas of the neocortex. Columnar defining factors in homotypical areas are generated, in part, within the cortex itself. The set of all modules composing such an entity may be fractionated into different modular subsets by different extrinsic connections. Linkages between them and subsets in other large entities form distributed systems. The neighborhood relations between connected subsets of modules in different entities result in nested distributed systems that serve distributed functions. A cortical area defined in classical cytoarchitectural terms may belong to more than one and sometimes to several distributed systems. Columns in cytoarchitectural areas located at some distance from one another, but with some common properties, may be linked by long-range, intracortical connections.},
	language = {eng},
	journal = {Brain: A Journal of Neurology},
	author = {Mountcastle, V. B.},
	month = apr,
	year = {1997},
	pmid = {9153131},
	keywords = {Animals, Brain Mapping, Cell Division, Cell Movement, Cerebral Cortex, Humans, Models, Neurological, Neurons},
	pages = {701--722},
}

@article{horton_cortical_2005,
	title = {The cortical column: a structure without a function},
	volume = {360},
	issn = {0962-8436},
	shorttitle = {The cortical column},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1569491/},
	doi = {10.1098/rstb.2005.1623},
	abstract = {This year, the field of neuroscience celebrates the 50th anniversary of Mountcastle's discovery of the cortical column. In this review, we summarize half a century of research and come to the disappointing realization that the column may have no function. Originally, it was described as a discrete structure, spanning the layers of the somatosensory cortex, which contains cells responsive to only a single modality, such as deep joint receptors or cutaneous receptors. Subsequently, examples of columns have been uncovered in numerous cortical areas, expanding the original concept to embrace a variety of different structures and principles. A ‘column’ now refers to cells in any vertical cluster that share the same tuning for any given receptive field attribute. In striate cortex, for example, cells with the same eye preference are grouped into ocular dominance columns. Unaccountably, ocular dominance columns are present in some species, but not others. In principle, it should be possible to determine their function by searching for species differences in visual performance that correlate with their presence or absence. Unfortunately, this approach has been to no avail; no visual faculty has emerged that appears to require ocular dominance columns. Moreover, recent evidence has shown that the expression of ocular dominance columns can be highly variable among members of the same species, or even in different portions of the visual cortex in the same individual. These observations deal a fatal blow to the idea that ocular dominance columns serve a purpose. More broadly, the term ‘column’ also denotes the periodic termination of anatomical projections within or between cortical areas. In many instances, periodic projections have a consistent relationship with some architectural feature, such as the cytochrome oxidase patches in V1 or the stripes in V2. These tissue compartments appear to divide cells with different receptive field properties into distinct processing streams. However, it is unclear what advantage, if any, is conveyed by this form of columnar segregation. Although the column is an attractive concept, it has failed as a unifying principle for understanding cortical function. Unravelling the organization of the cerebral cortex will require a painstaking description of the circuits, projections and response properties peculiar to cells in each of its various areas.},
	number = {1456},
	urldate = {2019-05-31},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Horton, Jonathan C and Adams, Daniel L},
	month = apr,
	year = {2005},
	pmid = {15937015},
	pmcid = {PMC1569491},
	pages = {837--862},
}

@article{haueis_life_2016,
	title = {The life of the cortical column: opening the domain of functional architecture of the cortex (1955–1981)},
	volume = {38},
	issn = {0391-9714},
	shorttitle = {The life of the cortical column},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4914527/},
	doi = {10.1007/s40656-016-0103-4},
	abstract = {The concept of the cortical column refers to vertical cell bands with similar response properties, which were initially observed by Vernon Mountcastle’s mapping of single cell recordings in the cat somatic cortex. It has subsequently guided over 50 years of neuroscientific research, in which fundamental questions about the modularity of the cortex and basic principles of sensory information processing were empirically investigated. Nevertheless, the status of the column remains controversial today, as skeptical commentators proclaim that the vertical cell bands are a functionally insignificant by-product of ontogenetic development. This paper inquires how the column came to be viewed as an elementary unit of the cortex from Mountcastle’s discovery in 1955 until David Hubel and Torsten Wiesel’s reception of the Nobel Prize in 1981. I first argue that Mountcastle’s vertical electrode recordings served as criteria for applying the column concept to electrophysiological data. In contrast to previous authors, I claim that this move from electrophysiological data to the phenomenon of columnar responses was concept-laden, but not theory-laden. In the second part of the paper, I argue that Mountcastle’s criteria provided Hubel Wiesel with a conceptual outlook, i.e. it allowed them to anticipate columnar patterns in the cat and macaque visual cortex. I argue that in the late 1970s, this outlook only briefly took a form that one could call a ‘theory’ of the cerebral cortex, before new experimental techniques started to diversify column research. I end by showing how this account of early column research fits into a larger project that follows the conceptual development of the column into the present.},
	number = {3},
	urldate = {2019-05-31},
	journal = {History and Philosophy of the Life Sciences},
	author = {Haueis, Philipp},
	year = {2016},
	pmid = {27325058},
	pmcid = {PMC4914527},
}

@article{defelipe_neocortical_2012,
	title = {The {Neocortical} {Column}},
	volume = {6},
	issn = {1662-5129},
	url = {https://www.frontiersin.org/articles/10.3389/fnana.2012.00022/full},
	doi = {10.3389/fnana.2012.00022},
	abstract = {The Neocortical Column},
	language = {English},
	urldate = {2019-05-31},
	journal = {Frontiers in Neuroanatomy},
	author = {Defelipe, Javier and Markram, Henry and Rockland, Kathleen S.},
	year = {2012},
	keywords = {Cerebral Cortex, cortical processes, cortical unit, macrocolumn’s function, minicolumns, neocortical column},
}

@incollection{sabour_dynamic_2017,
	title = {Dynamic {Routing} {Between} {Capsules}},
	url = {http://papers.nips.cc/paper/6975-dynamic-routing-between-capsules.pdf},
	urldate = {2019-05-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {3856--3866},
}

@inproceedings{sabour_matrix_2018,
	title = {Matrix capsules with {EM} routing},
	author = {Sabour, Sara and Frosst, Nicholas and Hinton, G},
	year = {2018},
}

@incollection{nachum_data-efficient_2018,
	title = {Data-{Efficient} {Hierarchical} {Reinforcement} {Learning}},
	url = {http://papers.nips.cc/paper/7591-data-efficient-hierarchical-reinforcement-learning.pdf},
	urldate = {2019-05-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Nachum, Ofir and Gu, Shixiang (Shane) and Lee, Honglak and Levine, Sergey},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {3303--3313},
}

@inproceedings{xiong_application_2018,
	title = {Application of {Transfer} {Learning} in {Continuous} {Time} {Series} for {Anomaly} {Detection} in {Commercial} {Aircraft} {Flight} {Data}},
	doi = {10.1109/SmartCloud.2018.00011},
	abstract = {In recent years, transfer learning has attracted widespread attention and research. Transfer learning is a new machine learning method that uses existing knowledge to solve different but related domain problems. It relaxes two basic assumptions in traditional machine learning: (1) Training samples for learning and new test samples satisfy the conditions of independent and identical distribution; (2) There must be enough training samples available to learn a good model. Since it is costly and dangerous to repeat testing flights at extreme conditions, building an anomaly detection model for aircraft flight is also constrained by insufficient samples in limited data for different testing flight scenarios. To handle the insufficient samples, we propose a transfer-learning based approach to establishing an anomaly detection model for dangerous actions of aircraft testing flights. In our approach, we transfer the "knowledge" obtained in some testing scenarios to other scenarios containing dangerous action. Evaluation results indicate that our approach works well in terms of convincing accuracy in prediction by models in target scenarios transferred from source scenarios.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Smart} {Cloud} ({SmartCloud})},
	author = {Xiong, P. and Zhu, Y. and Sun, Z. and Cao, Z. and Wang, M. and Zheng, Y. and Hou, J. and Huang, T. and Que, Z.},
	month = sep,
	year = {2018},
	keywords = {Aircraft, Anomaly detection, Atmospheric modeling, Electronic mail, Machine learning, Task analysis, Training, aerospace computing, aircraft, aircraft testing, aircraft testing flights, anomaly detection, anomaly detection model, commercial aircraft flight data, continuous time series, learning (artificial intelligence), machine learning method, security of data, time series, transfer learning},
	pages = {13--18},
}

@article{losing_incremental_2018,
	title = {Incremental on-line learning: {A} review and comparison of state of the art algorithms},
	volume = {275},
	issn = {0925-2312},
	shorttitle = {Incremental on-line learning},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231217315928},
	doi = {10.1016/j.neucom.2017.06.084},
	abstract = {Recently, incremental and on-line learning gained more attention especially in the context of big data and learning from data streams, conflicting with the traditional assumption of complete data availability. Even though a variety of different methods are available, it often remains unclear which of them is suitable for a specific task and how they perform in comparison to each other. We analyze the key properties of eight popular incremental methods representing different algorithm classes. Thereby, we evaluate them with regards to their on-line classification error as well as to their behavior in the limit. Further, we discuss the often neglected issue of hyperparameter optimization specifically for each method and test how robustly it can be done based on a small set of examples. Our extensive evaluation on data sets with different characteristics gives an overview of the performance with respect to accuracy, convergence speed as well as model complexity, facilitating the choice of the best method for a given application.},
	urldate = {2019-05-30},
	journal = {Neurocomputing},
	author = {Losing, Viktor and Hammer, Barbara and Wersing, Heiko},
	month = jan,
	year = {2018},
	keywords = {Data streams, Hyperparameter optimization, Incremental learning, Model selection, On-line learning},
	pages = {1261--1274},
}

@misc{vinyals_alphastar:_2019,
	title = {{AlphaStar}: {Mastering} the {Real}-{Time} {Strategy} {Game} {StarCraft} {II}},
	url = {https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/},
	author = {Vinyals, Oriol and Babuschkin, Igor and Chung, Junyoung and Mathieu, Michael and Silver, David},
	year = {2019},
}

@article{mohammadi_deep_2018,
	title = {Deep {Learning} for {IoT} {Big} {Data} and {Streaming} {Analytics}: {A} {Survey}},
	volume = {20},
	issn = {1553-877X},
	shorttitle = {Deep {Learning} for {IoT} {Big} {Data} and {Streaming} {Analytics}},
	doi = {10.1109/COMST.2018.2844341},
	abstract = {In the era of the Internet of Things (IoT), an enormous amount of sensing devices collect and/or generate various sensory data over time for a wide range of fields and applications. Based on the nature of the application, these devices will result in big or fast/real-time data streams. Applying analytics over such data streams to discover new information, predict future insights, and make control decisions is a crucial process that makes IoT a worthy paradigm for businesses and a quality-of-life improving technology. In this paper, we provide a thorough overview on using a class of advanced machine learning techniques, namely deep learning (DL), to facilitate the analytics and learning in the IoT domain. We start by articulating IoT data characteristics and identifying two major treatments for IoT data from a machine learning perspective, namely IoT big data analytics and IoT streaming data analytics. We also discuss why DL is a promising approach to achieve the desired analytics in these types of data and applications. The potential of using emerging DL techniques for IoT data analytics are then discussed, and its promises and challenges are introduced. We present a comprehensive background on different DL architectures and algorithms. We also analyze and summarize major reported research attempts that leveraged DL in the IoT domain. The smart IoT devices that have incorporated DL in their intelligence background are also discussed. DL implementation approaches on the fog and cloud centers in support of IoT applications are also surveyed. Finally, we shed light on some challenges and potential directions for future research. At the end of each section, we highlight the lessons learned based on our experiments and review of the recent literature.},
	number = {4},
	journal = {IEEE Communications Surveys Tutorials},
	author = {Mohammadi, M. and Al-Fuqaha, A. and Sorour, S. and Guizani, M.},
	year = {2018},
	keywords = {Big Data, DL implementation, Data analysis, Data mining, Deep learning, Economics, Internet of Things, IoT applications, IoT big data, IoT big data analytics, IoT data characteristics, IoT domain, Machine learning, Tutorials, advanced machine learning techniques, cloud centers, cloud-based analytics, data analysis, deep learning, deep neural network, fast data analytics, fast/real-time data streams, fog centers, learning (artificial intelligence), machine learning perspective, on-device intelligence, quality-of-life, sensory data, smart IoT devices, streaming analytics},
	pages = {2923--2960},
}

@misc{noauthor_deep_nodate,
	title = {Deep {Learning} for {IoT} {Big} {Data} and {Streaming} {Analytics}: {A} {Survey} - {IEEE} {Journals} \& {Magazine}},
	url = {https://ieeexplore.ieee.org/abstract/document/8373692},
	urldate = {2019-05-30},
}

@article{regier_learning_2016,
	title = {Learning an {Astronomical} {Catalog} of the {Visible} {Universe} through {Scalable} {Bayesian} {Inference}},
	url = {http://arxiv.org/abs/1611.03404},
	abstract = {Celeste is a procedure for inferring astronomical catalogs that attains state-of-the-art scientific results. To date, Celeste has been scaled to at most hundreds of megabytes of astronomical images: Bayesian posterior inference is notoriously demanding computationally. In this paper, we report on a scalable, parallel version of Celeste, suitable for learning catalogs from modern large-scale astronomical datasets. Our algorithmic innovations include a fast numerical optimization routine for Bayesian posterior inference and a statistically efficient scheme for decomposing astronomical optimization problems into subproblems. Our scalable implementation is written entirely in Julia, a new high-level dynamic programming language designed for scientific and numerical computing. We use Julia's high-level constructs for shared and distributed memory parallelism, and demonstrate effective load balancing and efficient scaling on up to 8192 Xeon cores on the NERSC Cori supercomputer.},
	urldate = {2019-05-29},
	journal = {arXiv:1611.03404 [astro-ph, stat]},
	author = {Regier, Jeffrey and Pamnany, Kiran and Giordano, Ryan and Thomas, Rollin and Schlegel, David and McAuliffe, Jon and Prabhat},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.03404},
	keywords = {85A35 (Primary), 68W10, 62P35, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, D.1.3, D.2, G.3, I.2, J.2, Statistics - Applications, Statistics - Machine Learning},
}

@article{bezanson_julia:_2017,
	title = {Julia: {A} {Fresh} {Approach} to {Numerical} {Computing}},
	volume = {59},
	issn = {0036-1445},
	shorttitle = {Julia},
	url = {https://epubs.siam.org/doi/10.1137/141000671},
	doi = {10.1137/141000671},
	abstract = {Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical  computing. Julia is  designed to be easy and fast and questions notions generally held to be “laws of nature"  by practitioners of numerical computing: {\textbackslash}beginlist {\textbackslash}item  High-level dynamic programs have to be slow. {\textbackslash}item  One must prototype in one language and then rewrite in another language for speed or deployment. {\textbackslash}item There are parts of a system appropriate for the programmer, and other parts that are best left untouched as they have been built by the experts. {\textbackslash}endlist We introduce the  Julia programming language and its design---a  dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch,  a  technique from computer science, picks  the right algorithm for the right circumstance. Abstraction, which is what good computation is really about, recognizes what remains the same after differences are stripped away. Abstractions in mathematics are captured as code through another technique from computer science, generic programming. Julia shows that  one can achieve machine performance without sacrificing human convenience.},
	number = {1},
	urldate = {2019-05-29},
	journal = {SIAM Review},
	author = {Bezanson, J. and Edelman, A. and Karpinski, S. and Shah, V.},
	month = jan,
	year = {2017},
	pages = {65--98},
}

@article{doya_what_1999,
	title = {What are the computations of the cerebellum, the basal ganglia and the cerebral cortex?},
	volume = {12},
	issn = {1879-2782},
	abstract = {The classical notion that the cerebellum and the basal ganglia are dedicated to motor control is under dispute given increasing evidence of their involvement in non-motor functions. Is it then impossible to characterize the functions of the cerebellum, the basal ganglia and the cerebral cortex in a simplistic manner? This paper presents a novel view that their computational roles can be characterized not by asking what are the "goals" of their computation, such as motor or sensory, but by asking what are the "methods" of their computation, specifically, their learning algorithms. There is currently enough anatomical, physiological, and theoretical evidence to support the hypotheses that the cerebellum is a specialized organism for supervised learning, the basal ganglia are for reinforcement learning, and the cerebral cortex is for unsupervised learning.This paper investigates how the learning modules specialized for these three kinds of learning can be assembled into goal-oriented behaving systems. In general, supervised learning modules in the cerebellum can be utilized as "internal models" of the environment. Reinforcement learning modules in the basal ganglia enable action selection by an "evaluation" of environmental states. Unsupervised learning modules in the cerebral cortex can provide statistically efficient representation of the states of the environment and the behaving system. Two basic action selection architectures are shown, namely, reactive action selection and predictive action selection. They can be implemented within the anatomical constraint of the network linking these structures. Furthermore, the use of the cerebellar supervised learning modules for state estimation, behavioral simulation, and encapsulation of learned skill is considered. Finally, the usefulness of such theoretical frameworks in interpreting brain imaging data is demonstrated in the paradigm of procedural learning.},
	language = {eng},
	number = {7-8},
	journal = {Neural Networks: The Official Journal of the International Neural Network Society},
	author = {Doya, K.},
	month = oct,
	year = {1999},
	pmid = {12662639},
	pages = {961--974},
}

@misc{noauthor_fundamentals_nodate,
	title = {The {Fundamentals} of {Brain} {Development}: {Integrating} {Nature} and {Nurture} - {Joan} {Stiles}, {Emeritus} {Professor} of {Cognitive} {Sciences} {Joan} {Stiles} - {Google} {Books}},
	url = {https://books.google.gr/books/about/The_Fundamentals_of_Brain_Development.html?id=BAbSGxIINYoC&printsec=frontcover&source=kp_read_button&redir_esc=y#v=onepage&q&f=false},
	urldate = {2019-05-29},
}

@book{paton_computation_2013,
	title = {Computation in {Cells} and {Tissues}: {Perspectives} and {Tools} of {Thought}},
	isbn = {978-3-662-06369-9},
	shorttitle = {Computation in {Cells} and {Tissues}},
	abstract = {The field of biologically inspired computation has coexisted with mainstream computing since the 1930s, and the pioneers in this area include Warren McCulloch, Walter Pitts, Robert Rosen, Otto Schmitt, Alan Turing, John von Neumann and Norbert Wiener. Ideas arising out of studies of biology have permeated algorithmics, automata theory, artificial intelligence, graphics, information systems and software design. Within this context, the biomolecular, cellular and tissue levels of biological organisation have had a considerable inspirational impact on the development of computational ideas. Such innovations include neural computing, systolic arrays, genetic and immune algorithms, cellular automata, artificial tissues, DNA computing and protein memories. With the rapid growth in biological knowledge there remains a vast source of ideas yet to be tapped. This includes developments associated with biomolecular, genomic, enzymic, metabolic, signalling and developmental systems and the various impacts on distributed, adaptive, hybrid and emergent computation. This multidisciplinary book brings together a collection of chapters by biologists, computer scientists, engineers and mathematicians who were drawn together to examine the ways in which the interdisciplinary displacement of concepts and ideas could develop new insights into emerging computing paradigms. Funded by the UK Engineering and Physical Sciences Research Council (EPSRC), the CytoCom Network formally met on five occasions to examine and discuss common issues in biology and computing that could be exploited to develop emerging models of computation.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Paton, R. and Bolouri, Hamid and Holcombe, W. Michael L. and Parish, J. Howard and Tateson, Richard},
	month = mar,
	year = {2013},
	note = {Google-Books-ID: h8eqCAAAQBAJ},
	keywords = {Computers / Computer Science, Computers / Computer Simulation, Computers / Information Technology, Computers / Intelligence (AI) \& Semantics, Computers / Machine Theory, Computers / User Interfaces, Mathematics / Applied, Science / Life Sciences / General},
}

@misc{noauthor_computation_nodate,
	title = {Computation in {Cells} and {Tissues}},
	url = {https://blackwells.co.uk/bookshop/product/Computation-in-Cells-and-Tissues-by-Ray-Paton/9783642055690},
	abstract = {The field of biologically inspired computation has coexisted with mainstream computing since the 1930s, and the pioneers in this area include Warren McCulloch, Walter Pitts, Robert Rosen, Otto Schmitt, Alan Turing, John von Neumann and Norbert Wiener. Ideas arising out of studies of biology have permeated algorithmics, automata theory, artificial intelligence, graphics, information systems and software design. Within this context, the biomolecular, cellular and tissue levels of biological organisation have had a considerable inspirational impact on the development of computational ideas. Such innovations include neural computing, systolic arrays, genetic and immune algorithms, cellular automata, artificial tissues, DNA computing and protein memories. With the rapid growth in biological knowledge there remains a vast source of ideas yet to be tapped. This includes developments associated with biomolecular, genomic, enzymic, metabolic, signalling and developmental systems and the various impacts on distributed, adaptive, hybrid and emergent computation. This multidisciplinary book brings together a collection of chapters by biologists, computer scientists, engineers and mathematicians who were drawn together to examine the ways in which the interdisciplinary displacement of concepts and ideas could develop new insights into emerging computing paradigms. Funded by the UK Engineering and Physical Sciences Research Council (EPSRC), the CytoCom Network formally met on five occasions to examine and discuss common issues in biology and computing that could be exploited to develop emerging models of computation.},
	language = {en},
	urldate = {2019-05-29},
}

@book{adamatzky_advances_2016,
	series = {Emergence, {Complexity} and {Computation}},
	title = {Advances in {Physarum} {Machines}: {Sensing} and {Computing} with {Slime} {Mould}},
	isbn = {978-3-319-26661-9},
	shorttitle = {Advances in {Physarum} {Machines}},
	url = {https://www.springer.com/gp/book/9783319266619},
	abstract = {This book is devoted to Slime mould Physarum polycephalum, which is a large single cell capable for distributed sensing, concurrent information processing, parallel computation and decentralized actuation. The ease of culturing and experimenting with Physarum makes this slime mould an ideal substrate for real-world implementations of unconventional sensing and computing devices The book is a treatise of theoretical and experimental laboratory studies on sensing and computing properties of slime mould, and on the development of mathematical and logical theories of Physarum behavior. It is shown how to make logical gates and circuits, electronic devices (memristors, diodes, transistors, wires, chemical and tactile sensors) with the slime mould. The book demonstrates how to modify properties of Physarum computing circuits with functional nano-particles and polymers, to interface the slime mould with field-programmable arrays, and to use Physarum as a controller of microbial fuel cells. A unique multi-agent model of slime is shown to serve well as a software slime mould capable for solving problems of computational geometry and graph optimization. The multiagent model is complemented by cellular automata models with parallel accelerations. Presented mathematical models inspired by Physarum include non-quantum implementation of Shor's factorization, structural learning, computation of shortest path tree on dynamic graphs, supply chain network design, p-adic computing and syllogistic reasoning. The book is a unique composition of vibrant and lavishly illustrated essays which will inspire scientists, engineers and artists to exploit natural phenomena in designs of future and emergent computing and sensing devices. It is a 'bible' of experimental computing with spatially extended living substrates, it spanstopics from biology of slime mould, to bio-sensing, to unconventional computing devices and robotics, non-classical logics and music and arts.},
	language = {en},
	urldate = {2019-05-29},
	publisher = {Springer International Publishing},
	editor = {Adamatzky, Andrew},
	year = {2016},
}

@article{anderson_more_1972,
	title = {More {Is} {Different}},
	volume = {177},
	copyright = {© 1972},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/177/4047/393},
	doi = {10.1126/science.177.4047.393},
	language = {en},
	number = {4047},
	urldate = {2019-05-29},
	journal = {Science},
	author = {Anderson, P. W.},
	month = aug,
	year = {1972},
	pmid = {17796623},
	pages = {393--396},
}

@incollection{brading_symmetry_2017,
	edition = {Winter 2017},
	title = {Symmetry and {Symmetry} {Breaking}},
	url = {https://plato.stanford.edu/archives/win2017/entries/symmetry-breaking/},
	abstract = {Symmetry considerations dominate modern fundamental physics, both inquantum theory and in relativity. Philosophers are now beginning todevote increasing attention to such issues as the significance ofgauge symmetry, quantum particle identity in the light of permutationsymmetry, how to make sense of parity violation, the role of symmetrybreaking, the empirical status of symmetry principles, and so forth.These issues relate directly to traditional problems in the philosophyof science, including the status of the laws of nature, therelationships between mathematics, physical theory, and the world, andthe extent to which mathematics suggests new physics., This entry begins with a brief description of the historical roots andemergence of the concept of symmetry that is at work in modernscience. It then turns to the application of this concept to physics,distinguishing between two different uses of symmetry: symmetryprinciples versus symmetry arguments. It mentions the differentvarieties of physical symmetries, outlining the ways in which theywere introduced into physics. Then, stepping back from the details ofthe various symmetries, it makes some remarks of a general natureconcerning the status and significance of symmetries in physics.},
	urldate = {2019-05-29},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Brading, Katherine and Castellani, Elena and Teh, Nicholas},
	editor = {Zalta, Edward N.},
	year = {2017},
}

@book{piaget_origins_1952,
	address = {New York, NY, US},
	series = {The origins of intelligence in children},
	title = {The origins of intelligence in children},
	abstract = {This work, a second edition of which has very kindly been requested, was followed by La Construction du réel chez l'enfant and was to have been completed by a study of the genesis of imitation in the child. The latter piece of research, whose publication we have postponed because it is so closely connected with the analysis of play and representational symbolism, appeared in 1945, inserted in a third work, La formation du symbole chez l'enfant. Together these three works form one entity dedicated to the beginnings of intelligence, that is to say, to the various manifestations of sensorimotor intelligence and to the most elementary forms of expression. The theses developed in this volume, which concern in particular the formation of the sensorimotor schemata and the mechanism of mental assimilation, have given rise to much discussion which pleases us and prompts us to thank both our opponents and our sympathizers for their kind interest in our work. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	publisher = {W W Norton \& Co},
	author = {Piaget, Jean},
	editor = {Cook, Margaret},
	year = {1952},
	doi = {10.1037/11494-000},
	keywords = {Assimilation (Cognitive Process), Childhood Play Development, Cognitive Development, Imitation (Learning), Intelligence, Nonverbal Communication, Perceptual Motor Development, Symbolism},
}

@book{minsky_society_1988,
	title = {Society {Of} {Mind}},
	isbn = {978-0-671-65713-0},
	abstract = {Marvin Minsky -- one of the fathers of computer science and cofounder of the Artificial Intelligence Laboratory at MIT -- gives a revolutionary answer to the age-old question: "How does the mind work?"  Minsky brilliantly portrays the mind as a "society" of tiny components that are themselves mindless. Mirroring his theory, Minsky boldly casts The Society of Mind as an intellectual puzzle whose pieces are assembled along the way. Each chapter -- on a self-contained page -- corresponds to a piece in the puzzle. As the pages turn, a unified theory of the mind emerges, like a mosaic. Ingenious, amusing, and easy to read, The Society of Mind is an adventure in imagination.},
	language = {en},
	publisher = {Simon and Schuster},
	author = {Minsky, Marvin},
	month = mar,
	year = {1988},
	note = {Google-Books-ID: bLDLllfRpdkC},
	keywords = {Psychology / Cognitive Psychology \& Cognition, Science / General, Science / Philosophy \& Social Aspects},
}

@article{lenat_thresholds_1991,
	title = {On the thresholds of knowledge},
	volume = {47},
	issn = {0004-3702},
	url = {http://www.sciencedirect.com/science/article/pii/000437029190055O},
	doi = {10.1016/0004-3702(91)90055-O},
	abstract = {We articulate the three major findings and hypotheses of AI to date: 1.(1) The Knowledge Principle: If a program is to perform a complex task well, it must know a great deal about the world in which it operates. In the absence of knowledge, all you have left is search and reasoning, and that isn't enough.2.(2) The Breadth Hypothesis: To behave intelligently in unexpected situations, an agent must be capable of falling back on increasingly general knowledge and analogizing to specific but superficially far-flung knowledge. (This is an extension of the preceding principle.)3.(3) AI as Empirical Inquiry: Premature mathematization, or focusing on toy problems, washes out details from reality that later turn out to be significant. Thus, we must test our ideas experimentally, falsifiably, on large problems. We present evidence for these propositions, contrast them with other strategic approaches to AI, point out their scope and limitations, and discuss the future directions they mandate for the main enterprise of AI research.},
	number = {1},
	urldate = {2019-05-28},
	journal = {Artificial Intelligence},
	author = {Lenat, Douglas B. and Feigenbaum, Edward A.},
	month = jan,
	year = {1991},
	pages = {185--250},
}

@inproceedings{legg_collection_2007,
	title = {A {Collection} of {Definitions} of {Intelligence}},
	booktitle = {Advances in {Artificial} {General} {Intelligence}: {Concepts}, {Architectures} and {Algorithms}},
	publisher = {IOS Press},
	author = {Legg, Shane and Hutter, Marcus},
	year = {2007},
	pages = {17--24},
}

@book{goertzel_advances_2007,
	title = {Advances in {Artificial} {General} {Intelligence}: {Concepts}, {Architectures} and {Algorithms} : {Proceedings} of the {AGI} {Workshop} 2006},
	isbn = {978-1-58603-758-1},
	shorttitle = {Advances in {Artificial} {General} {Intelligence}},
	abstract = {"The topic of this book the creation of software programs displaying broad, deep, human-style general intelligence is a grand and ambitious one. And yet it is far from a frivolous one: what the papers in this publication illustrate is that it is a fit and proper subject for serious science and engineering exploration. No one has yet created a software program with human-style or (even roughly) human-level general intelligence but we now have a sufficiently rich intellectual toolkit that it is possible to think about such a possibility in detail, and make serious attempts at design, analysis and engineering. possibility in detail, and make serious attempts at design, analysis and engineering. This is the situation that led to the organization of the 2006 AGIRI (Artificial General Intelligence Research Institute) workshop; and to the decision to publish a book from contributions by the speakers at the conference.The material presented here only scratches the surface of the AGI-related R\&D work that is occurring around the world at this moment. But the editors are pleased to have had the chance to be involved in organizing and presenting at least a small percentage of the contemporary progress."},
	language = {en},
	publisher = {IOS Press},
	author = {Goertzel, Ben and Wang, Pei},
	year = {2007},
	note = {Google-Books-ID: t2G5srpFRhEC},
	keywords = {Computers / Intelligence (AI) \& Semantics},
}

@misc{chazelle_natural_nodate,
	title = {Natural {Algorithms} and {Influence} {Systems}},
	url = {https://cacm.acm.org/magazines/2012/12/157889-natural-algorithms-and-influence-systems/abstract},
	abstract = {Algorithms lay the grounds for numerical simulations and, crucially, provide a powerful framework for their analysis. The new area of natural algorithms may reprise in the life sciences the role differential equations have long played in the physical sciences.},
	language = {en},
	urldate = {2019-05-28},
	author = {Chazelle, Bernard},
}

@techreport{wang_working_1995,
	title = {On the {Working} {Definition} of {Intelligence}},
	abstract = {This paper is about the philosophical and methodological foundation of artificial intelligence (AI). After discussing what is a good "working definition", "intelligence" is defined as "the ability for an information processing system to adapt to its environment with insufficient knowledge and resources". Applying the definition to a reasoning system, we get the major components of Non-Axiomatic Reasoning System (NARS),  which is a symbolic logic implemented in a computer system, and has many interesting properties that are closely related to intelligence. The definition also clarifies the difference and relationship between AI and other disciplines, such as computer science. Finally, the definition is compared with other popular definitions of intelligence, and its advantages are argued. 1 To Define Intelligence  1.1 Retrospect  The attempts of clarifying the concept "intelligence" and discussing the possibility and paths to produce it in computing machinery can be backtracked to Turin...},
	author = {Wang, Pei},
	year = {1995},
	keywords = {**},
}

@article{bengio_towards_2015,
	title = {Towards {Biologically} {Plausible} {Deep} {Learning}},
	url = {http://arxiv.org/abs/1502.04156},
	abstract = {Neuroscientists have long criticised deep learning algorithms as incompatible with current knowledge of neurobiology. We explore more biologically plausible versions of deep representation learning, focusing here mostly on unsupervised learning but developing a learning mechanism that could account for supervised, unsupervised and reinforcement learning. The starting point is that the basic learning rule believed to govern synaptic weight updates (Spike-Timing-Dependent Plasticity) arises out of a simple update rule that makes a lot of sense from a machine learning point of view and can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective function (be it supervised, unsupervised, or reward-driven). The second main idea is that this corresponds to a form of the variational EM algorithm, i.e., with approximate rather than exact posteriors, implemented by neural dynamics. Another contribution of this paper is that the gradients required for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Finally, we extend the theory about the probabilistic interpretation of auto-encoders to justify improved sampling schemes based on the generative interpretation of denoising auto-encoders, and we validate all these ideas on generative learning tasks.},
	urldate = {2019-05-28},
	journal = {arXiv:1502.04156 [cs]},
	author = {Bengio, Yoshua and Lee, Dong-Hyun and Bornschein, Jorg and Mesnard, Thomas and Lin, Zhouhan},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.04156},
	keywords = {*, Computer Science - Machine Learning},
}

@article{wang_cognitive_nodate,
	title = {Cognitive {Logic} versus {Mathematical} {Logic}},
	author = {Wang, Pei},
	keywords = {*},
}

@book{piaget_jean_origins_nodate,
	title = {The origins of intelligence in children},
	author = {Piaget, Jean},
	keywords = {*},
}

@misc{piaget_jean_psychology_nodate,
	title = {The {Psychology} of {Intelligence}},
	url = {https://www.goodreads.com/work/best_book/13123281-the-psychology-of-intelligence},
	abstract = {Think of developmental psychology, and the name of Jean Piaget immediately springs to mind. His theory of learning lies at the very heart...},
	urldate = {2019-05-28},
	author = {Piaget, Jean},
}

@incollection{wang_logic_2007,
	address = {Berlin, Heidelberg},
	series = {Cognitive {Technologies}},
	title = {The {Logic} of {Intelligence}},
	isbn = {978-3-540-68677-4},
	url = {https://doi.org/10.1007/978-3-540-68677-4_2},
	abstract = {SummaryIs there an “essence of intelligence” that distinguishes intelligent systems from non-intelligent systems? If there is, then what is it? This chapter suggests an answer to these questions by introducing the ideas behind the NARS (Nonaxiomatic Reasoning System) project. NARS is based on the opinion that the essence of intelligence is the ability to adapt with insufficient knowledge and resources. According to this belief, the author has designed a novel formal logic, and implemented it in a computer system. Such a “logic of intelligence” provides a unified explanation for many cognitive functions of the human mind, and is also concrete enough to guide the actual building of a general purpose “thinking machine”.},
	language = {en},
	urldate = {2019-05-28},
	booktitle = {Artificial {General} {Intelligence}},
	publisher = {Springer Berlin Heidelberg},
	author = {Wang, Pei},
	editor = {Goertzel, Ben and Pennachin, Cassio},
	year = {2007},
	doi = {10.1007/978-3-540-68677-4_2},
	keywords = {*, Human Mind, Inference Rule, Predicate Logic, Reasoning System, Turing Machine},
	pages = {31--62},
}

@incollection{dreyfus_making_1991,
	address = {London},
	series = {Artificial {Intelligence} and {Society}},
	title = {Making a {Mind} {Versus} {Modelling} the {Brain}: {Artificial} {Intelligence} {Back} at the {Branchpoint}},
	isbn = {978-1-4471-1776-6},
	shorttitle = {Making a {Mind} {Versus} {Modelling} the {Brain}},
	url = {https://doi.org/10.1007/978-1-4471-1776-6_3},
	abstract = {In the early 1950s, as calculating machines were coming into their own, a few pioneer thinkers began to realise that digital computers could be more than number-crunchers. At that point two opposed visions of what computers could be, each with its correlated research programme, emerged and struggled for recognition. One faction saw computers as a system for manipulating mental symbols; the other, as a medium for modelling the brain. One sought to use computers to instantiate a formal representation of the world; the other, to simulate the interactions of neurons. One took problem solving as its paradigm of intelligence; the other, learning. One utilised logic; the other, statistics. One school was the heir to the rationalist, reductionist tradition in philosophy; the other viewed itself as idealised, holistic neuroscience.},
	language = {en},
	urldate = {2019-05-28},
	booktitle = {Understanding the {Artificial}: {On} the {Future} {Shape} of {Artificial} {Intelligence}},
	publisher = {Springer London},
	author = {Dreyfus, Hubert L. and Dreyfus, Stuart E.},
	editor = {Negrotti, Massimo},
	year = {1991},
	doi = {10.1007/978-1-4471-1776-6_3},
	keywords = {Cognitive Science Society, Everyday World, Hide Node, Intelligent Behaviour, Symbolic Representation},
	pages = {33--54},
}

@article{roth_evolution_2005,
	title = {Evolution of the brain and intelligence},
	volume = {9},
	issn = {1364-6613},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661305000823},
	doi = {10.1016/j.tics.2005.03.005},
	abstract = {Intelligence has evolved many times independently among vertebrates. Primates, elephants and cetaceans are assumed to be more intelligent than ‘lower’ mammals, the great apes and humans more than monkeys, and humans more than the great apes. Brain properties assumed to be relevant for intelligence are the (absolute or relative) size of the brain, cortex, prefrontal cortex and degree of encephalization. However, factors that correlate better with intelligence are the number of cortical neurons and conduction velocity, as the basis for information-processing capacity. Humans have more cortical neurons than other mammals, although only marginally more than whales and elephants. The outstanding intelligence of humans appears to result from a combination and enhancement of properties found in non-human primates, such as theory of mind, imitation and language, rather than from ‘unique’ properties.},
	number = {5},
	urldate = {2019-05-28},
	journal = {Trends in Cognitive Sciences},
	author = {Roth, Gerhard and Dicke, Ursula},
	month = may,
	year = {2005},
	pages = {250--257},
}

@article{legg_universal_2007,
	title = {Universal {Intelligence}: {A} {Definition} of {Machine} {Intelligence}},
	volume = {17},
	issn = {1572-8641},
	shorttitle = {Universal {Intelligence}},
	url = {https://doi.org/10.1007/s11023-007-9079-x},
	doi = {10.1007/s11023-007-9079-x},
	abstract = {A fundamental problem in artificial intelligence is that nobody really knows what intelligence is. The problem is especially acute when we need to consider artificial systems which are significantly different to humans. In this paper we approach this problem in the following way: we take a number of well known informal definitions of human intelligence that have been given by experts, and extract their essential features. These are then mathematically formalised to produce a general measure of intelligence for arbitrary machines. We believe that this equation formally captures the concept of machine intelligence in the broadest reasonable sense. We then show how this formal definition is related to the theory of universal optimal learning agents. Finally, we survey the many other tests and definitions of intelligence that have been proposed for machines.},
	language = {en},
	number = {4},
	urldate = {2019-05-28},
	journal = {Minds and Machines},
	author = {Legg, Shane and Hutter, Marcus},
	month = dec,
	year = {2007},
	keywords = {AIXI, Complexity theory, Definitions, Intelligence, Intelligence tests, Measures, Theoretical foundations, Turing test},
	pages = {391--444},
}

@article{benedek_intelligence_2014,
	title = {Intelligence, creativity, and cognitive control: {The} common and differential involvement of executive functions in intelligence and creativity},
	volume = {46},
	issn = {0160-2896},
	shorttitle = {Intelligence, creativity, and cognitive control},
	url = {http://www.sciencedirect.com/science/article/pii/S0160289614000798},
	doi = {10.1016/j.intell.2014.05.007},
	abstract = {Intelligence and creativity are known to be correlated constructs suggesting that they share a common cognitive basis. The present study assessed three specific executive abilities – updating, shifting, and inhibition – and examined their common and differential relations to fluid intelligence and creativity (i.e., divergent thinking ability) within a latent variable model approach. Additionally, it was tested whether the correlation of fluid intelligence and creativity can be explained by a common executive involvement. As expected, fluid intelligence was strongly predicted by updating, but not by shifting or inhibition. Creativity was predicted by updating and inhibition, but not by shifting. Moreover, updating (and the personality factor openness) was found to explain a relevant part of the shared variance between intelligence and creativity. The findings provide direct support for the executive involvement in creative thought and shed further light on the functional relationship between intelligence and creativity.},
	urldate = {2019-05-28},
	journal = {Intelligence},
	author = {Benedek, Mathias and Jauk, Emanuel and Sommer, Markus and Arendasy, Martin and Neubauer, Aljoscha C.},
	month = sep,
	year = {2014},
	keywords = {Creativity, Divergent thinking, Executive control, Intelligence, Working memory, no},
	pages = {73--83},
}

@article{jauk_relationship_2013,
	title = {The relationship between intelligence and creativity: {New} support for the threshold hypothesis by means of empirical breakpoint detection},
	volume = {41},
	issn = {0160-2896},
	shorttitle = {The relationship between intelligence and creativity},
	url = {http://www.sciencedirect.com/science/article/pii/S016028961300024X},
	doi = {10.1016/j.intell.2013.03.003},
	abstract = {The relationship between intelligence and creativity has been subject to empirical research for decades. Nevertheless, there is yet no consensus on how these constructs are related. One of the most prominent notions concerning the interplay between intelligence and creativity is the threshold hypothesis, which assumes that above-average intelligence represents a necessary condition for high-level creativity. While earlier research mostly supported the threshold hypothesis, it has come under fire in recent investigations. The threshold hypothesis is commonly investigated by splitting a sample at a given threshold (e.g., at 120 IQ points) and estimating separate correlations for lower and upper IQ ranges. However, there is no compelling reason why the threshold should be fixed at an IQ of 120, and to date, no attempts have been made to detect the threshold empirically. Therefore, this study examined the relationship between intelligence and different indicators of creative potential and of creative achievement by means of segmented regression analysis in a sample of 297 participants. Segmented regression allows for the detection of a threshold in continuous data by means of iterative computational algorithms. We found thresholds only for measures of creative potential but not for creative achievement. For the former the thresholds varied as a function of criteria: When investigating a liberal criterion of ideational originality (i.e., two original ideas), a threshold was detected at around 100 IQ points. In contrast, a threshold of 120 IQ points emerged when the criterion was more demanding (i.e., many original ideas). Moreover, an IQ of around 85 IQ points was found to form the threshold for a purely quantitative measure of creative potential (i.e., ideational fluency). These results confirm the threshold hypothesis for qualitative indicators of creative potential and may explain some of the observed discrepancies in previous research. In addition, we obtained evidence that once the intelligence threshold is met, personality factors become more predictive for creativity. On the contrary, no threshold was found for creative achievement, i.e. creative achievement benefits from higher intelligence even at fairly high levels of intellectual ability.},
	number = {4},
	urldate = {2019-05-28},
	journal = {Intelligence},
	author = {Jauk, Emanuel and Benedek, Mathias and Dunst, Beate and Neubauer, Aljoscha C.},
	month = jul,
	year = {2013},
	keywords = {*, Breakpoint detection, Creativity, Intelligence, Segmented regression, Threshold hypothesis},
	pages = {212--221},
}

@article{chiel_brain_1997,
	title = {The brain has a body: adaptive behavior emerges from interactions of nervous system, body and environment},
	volume = {20},
	issn = {0166-2236},
	shorttitle = {The brain has a body},
	url = {http://www.sciencedirect.com/science/article/pii/S0166223697011491},
	doi = {10.1016/S0166-2236(97)01149-1},
	abstract = {Studies of mechanisms of adaptive behavior generally focus on neurons and circuits. But adaptive behavior also depends on interactions among the nervous system, body and environment: sensory preprocessing and motor post-processing filter inputs to and outputs from the nervous system; co-evolution and co-development of nervous system and periphery create matching and complementarity between them; body structure creates constraints and opportunities for neural control; and continuous feedback between nervous system, body and environment are essential for normal behavior. This broader view of adaptive behavior has been a major underpinning of ecological psychology and has influenced behavior-based robotics. Computational neuroethology, which jointly models neural control and periphery of animals, is a promising methodology for understanding adaptive behavior.},
	number = {12},
	urldate = {2019-05-28},
	journal = {Trends in Neurosciences},
	author = {Chiel, Hillel J. and Beer, Randall D.},
	month = dec,
	year = {1997},
	keywords = {adaptive behavior, behavior based robotics, biomechanics, computational neuroethology, coupled systems, dynamics, ecological psychology},
	pages = {553--557},
}

@article{damasio_nature_2013,
	title = {The nature of feelings: evolutionary and neurobiological origins},
	volume = {14},
	copyright = {2013 Nature Publishing Group},
	issn = {1471-0048},
	shorttitle = {The nature of feelings},
	url = {https://www.nature.com/articles/nrn3403},
	doi = {10.1038/nrn3403},
	abstract = {Feelings are mental experiences of body states. They signify physiological need (for example, hunger), tissue injury (for example, pain), optimal function (for example, well-being), threats to the organism (for example, fear or anger) or specific social interactions (for example, compassion, gratitude or love). Feelings constitute a crucial component of the mechanisms of life regulation, from simple to complex. Their neural substrates can be found at all levels of the nervous system, from individual neurons to subcortical nuclei and cortical regions.},
	language = {en},
	number = {2},
	urldate = {2019-05-28},
	journal = {Nature Reviews Neuroscience},
	author = {Damasio, Antonio and Carvalho, Gil B.},
	month = feb,
	year = {2013},
	pages = {143--152},
}

@book{damasio_descartes_2006,
	title = {Descartes' {Error}},
	isbn = {978-0-09-950164-0},
	abstract = {\&quot;Although I cannot tell for certain what sparked my interest in the neural underpinnings of reason, I do know when I became convinced that the traditional views on the nature of rationality could not be correct.\&quot; Thus begins a book that takes the reader on a journey of discovery, from the story of Phineas Gage, the famous nineteenth-century case of behavioral change that followed brain damage, to the contemporary recreation of Gage\&\#39;s brain; and from the doubts of a young neurologist to a testable hypothesis concerning the emotions and their fundamental role in rational human behavior. Drawing on his experiences with neurological patients affected by brain damage (his laboratory is recognized worldwide as the foremost center for the study of such patients), Antonio Damasio shows how the absence of emotion and feeling can break down rationality. In the course of explaining how emotions and feelings contribute to reason and to adaptive social behavior, Damasio also offers a novel perspective on what emotions and feelings actually are: a direct sensing of our own body states, a link between the body and its survival-oriented regulations, on the one hand, and consciousness, on the other. Descartes\&\#39; Error leads us to conclude that human organisms are endowed from the very beginning with a spirited passion for making choices, which the social mind can use to build rational behavior.},
	language = {en},
	publisher = {Vintage},
	author = {Damasio, Antonio R.},
	year = {2006},
	note = {Google-Books-ID: 5aczCwAAQBAJ},
}

@article{starzyk_motivation_2008,
	title = {Motivation in {Embodied} {Intelligence}},
	url = {https://www.intechopen.com/books/frontiers_in_robotics_automation_and_control/motivation_in_embodied_intelligence},
	doi = {10.5772/6332},
	abstract = {Open access peer-reviewed chapter},
	language = {en},
	urldate = {2019-05-28},
	journal = {Frontiers in Robotics, Automation and Control},
	author = {Starzyk, Janusz A.},
	month = oct,
	year = {2008},
}

@inproceedings{wang_what_2008,
	title = {What {Do} {You} {Mean} by “{AI}”?},
	isbn = {978-1-58603-833-5},
	url = {http://dl.acm.org/citation.cfm?id=1566174.1566207},
	urldate = {2019-05-28},
	publisher = {IOS Press},
	author = {Wang, Pei},
	month = jun,
	year = {2008},
	pages = {362--373},
}

@misc{noauthor_kb:connector_nodate,
	title = {kb:connector zotero unavailable [{Zotero} {Documentation}]},
	url = {https://www.zotero.org/support/kb/connector_zotero_unavailable},
	urldate = {2019-05-28},
}

@article{noauthor_notitle_nodate,
}
